Hello everyone, üßë‚Äçüíª

For this week's lab, we're focusing on the practical challenges and solutions involved in training RNNs. The following assignment requires you to connect the theoretical problem of exploding gradients to its practical solution within the context of a character-level text generation model.

<br>

## Assignment: Gradient Clipping in Character-Level RNNs

### Prompt

Prepare a technical report that addresses the following points regarding the training of Recurrent Neural Networks for text generation:

1.  **Exploding Gradients and Gradient Clipping**:
    * Explain the mathematical cause of the **exploding gradient problem** in RNNs, referencing the role of the recurrent weight matrix $W_{hh}$.
    * Describe the **L2 norm gradient clipping** algorithm step-by-step, including the relevant mathematical equations. Explain why it's crucial that this technique preserves the direction of the gradient.

2.  **Integration into the Training Loop**:
    * Outline the key steps in the training loop for a character-level RNN, starting from the forward pass for a sequence of characters.
    * Clearly identify the specific point within this loop where the gradient clipping algorithm is applied.

3.  **Training vs. Inference**:
    * Briefly contrast the objective and process of **training** the character-level RNN with the process of using it for **inference** (text generation).

<br>
<hr>
<br>

## Report: Gradient Clipping and the Implementation of a Character-Level RNN

### Introduction

While Recurrent Neural Networks (RNNs) are powerful models for sequential data, their training is often plagued by numerical instability. One of the primary challenges is the exploding gradient problem, which can completely halt the learning process. This report details the mathematical origin of this problem, explains its most common solution‚Äîgradient clipping‚Äîand describes how this technique is integrated into the practical training workflow of a character-level RNN for text generation.

<br>

### 1. The Exploding Gradient Problem and Gradient Clipping üí•

**Mathematical Cause of Exploding Gradients**

The exploding gradient problem arises during Backpropagation Through Time (BPTT). The gradient of the loss with respect to an early hidden state is calculated via the chain rule, which involves the repeated multiplication of the Jacobian matrix $\frac{\partial h_t}{\partial h_{t-1}}$. This Jacobian is a function of the recurrent weight matrix $W_{hh}$.

When the gradient signal is propagated back through $k$ time steps, the calculation involves a term proportional to $(W_{hh})^k$. If the largest absolute eigenvalue (spectral radius) of $W_{hh}$ is greater than 1, the norm of this matrix power grows exponentially with $k$. This causes the gradients to become exceedingly large, leading to massive, unstable updates to the model's weights.

**The Gradient Clipping Algorithm**

Gradient clipping is a direct and effective technique to counter this problem. It works by rescaling gradients if their overall magnitude exceeds a predefined threshold. The most common method is L2 norm clipping.

The algorithm is as follows:
1.  After computing the gradients for all model parameters $\theta$, denoted as $\nabla_{\theta} L$, we calculate their global L2 norm, $g$.
    $$
    g = ||\nabla_{\theta} L||_2 = \sqrt{\sum_{i} (\nabla_{\theta_i} L)^2}
    $$
2.  We compare this norm $g$ to a pre-set hyperparameter, the `threshold`, denoted by $\tau$.
3.  If the norm exceeds the threshold ($g > \tau$), the gradient vector is rescaled:
    $$
    \text{Clipped Gradient} = \nabla_{\theta} L \times \frac{\tau}{g}
    $$
    If $g \le \tau$, the gradient is left unchanged.
4.  The optimizer then uses this potentially clipped gradient for the weight update.



It is crucial that this process **preserves the direction** of the gradient vector. The direction of the gradient points towards the steepest ascent of the loss function, meaning its negative direction is the optimal path for minimization. Clipping only shortens the **step size** along this path, preventing the optimizer from making an update so large that it overshoots the target and destabilizes training.

<br>
<hr>
<br>

### 2. Integration into the Character-Level RNN Training Loop ‚öôÔ∏è

Gradient clipping is an integral part of the BPTT process. Here is a step-by-step outline of a single training iteration for a character-level RNN:

1.  **Forward Pass**: An input sequence of characters (e.g., "hell") is fed through the unrolled RNN, one character at a time. At each time step $t$, the model outputs a probability distribution, $p_t$, over the entire vocabulary for the next possible character.
2.  **Loss Calculation**: The total loss for the sequence, $\mathcal{L}$, is calculated by comparing the model's predictions ($p_1, p_2, ..., p_T$) against the true target sequence (e.g., "ello"). This is typically done using cross-entropy loss, summed or averaged over all time steps.
3.  **Backward Pass (BPTT)**: Standard backpropagation is performed on the unrolled network to compute the gradient of the total loss with respect to every parameter in the model, yielding the raw gradient vector $\nabla_{\theta} L$.
4.  **Gradient Clipping**: **This is the critical step where clipping is applied.** Immediately after the gradients are computed in the backward pass, and *before* they are fed to the optimizer, the L2 norm clipping algorithm described in Part 1 is executed on the gradient vector $\nabla_{\theta} L$.
5.  **Optimizer Step**: The optimizer (e.g., Adam) receives the (potentially clipped) gradients and performs a single update step on the model's weights.

This cycle is repeated for many batches of sequences until the model converges.

<br>
<hr>
<br>

### 3. Contrasting Training vs. Inference ‚úçÔ∏è

The usage of the RNN model differs significantly between training and inference.

* **Training**: The objective is to **learn** the statistical patterns of the language. The model is provided with both an input sequence and a corresponding target sequence. It processes the entire input sequence, compares its predictions to the target at every step to calculate a loss, and uses the resulting gradients to update its internal weights. This process is often called "teacher forcing" because the model is guided by the ground truth at each step.

* **Inference (Text Generation)**: The objective is to **create** new text. The model is given only an initial "seed" sequence (e.g., "The cat "). There is no target sequence and no loss calculation. The process is **auto-regressive**:
    1.  The model processes the seed to establish its hidden state.
    2.  It predicts a probability distribution for the character to follow the seed.
    3.  A character is **sampled** from this distribution.
    4.  This newly generated character is appended to the sequence and becomes the input for the next time step.
    5.  This loop repeats, with the model feeding its own output back to itself to generate text one character at a time.