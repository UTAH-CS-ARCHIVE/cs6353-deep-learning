Hello class, ðŸ§ 

This week, we explored the sophisticated architectures that solve the long-term dependency problem. The following assignment will test your understanding of the internal mechanics of the Long Short-Term Memory (LSTM) cell, which is the cornerstone of modern sequence modeling.

<br>

## Assignment: Dissecting the LSTM Architecture

### Prompt

Prepare a technical report that provides a detailed mathematical breakdown of the LSTM cell. Your report must address the following points:

1.  **The Three Gates**: Describe the purpose and mathematical equation for each of the three primary gates in an LSTM: the **Forget Gate** ($f_t$), the **Input Gate** ($i_t$), and the **Output Gate** ($o_t$). Explain what each gate controls within the information flow of the cell.
2.  **The Cell State Update**: Write down the full equation for the update of the LSTM's **cell state** (from $C_{t-1}$ to $C_t$). Explain how the outputs from the forget and input gates are used in this process.
3.  **Solving the Vanishing Gradient Problem**: Explain how the design of the cell state update mechanism, particularly its **additive** nature, helps to mitigate the vanishing gradient problem that is prevalent in simple RNNs.

<br>
<hr>
<br>

## Report: The Architecture and Gradient Flow of the LSTM Cell

### Introduction

The Long Short-Term Memory (LSTM) network is an advanced recurrent architecture designed specifically to overcome the long-term dependency problem inherent in simple RNNs. Its success lies in a sophisticated internal structure based on a dedicated cell state and a series of "gates" that regulate information flow. This report dissects the mathematical operations of these gates, details the crucial cell state update equation, and explains how this design enables the stable flow of gradients over long sequences.

<br>

### 1. The Gating Mechanisms of the LSTM ðŸšª

An LSTM cell uses three gates to control its memory. Each gate is a sigmoid neural network layer that outputs a vector with values between 0 and 1, which acts as a filter on information passing through it.

**a. The Forget Gate ($f_t$)**
This gate's purpose is to decide which information should be discarded from the previous cell state, $C_{t-1}$. It looks at the previous hidden state, $h_{t-1}$, and the current input, $x_t$, to make this decision.

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

A value of '1' in the output vector $f_t$ means "keep this information," while a '0' means "forget this information."

**b. The Input Gate ($i_t$)**
This gate determines which new information should be stored in the cell state. This is a two-part process:
1.  The sigmoid "input gate layer" decides which values to update.
    $$
    i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
    $$
2.  A `tanh` layer generates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state.
    $$
    \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
    $$
The input gate $i_t$ then scales the candidate values $\tilde{C}_t$ to determine the final update.

**c. The Output Gate ($o_t$)**
This gate controls what information from the cell state is used to compute the new hidden state, $h_t$. The hidden state is a filtered version of the cell state.
1.  The sigmoid layer determines which parts of the cell state will be output.
    $$
    o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
    $$
2.  This is then combined with the updated cell state to produce the final hidden state.
    $$
    h_t = o_t * \tanh(C_t)
    $$

<br>
<hr>
<br>

### 2. The Cell State Update Equation âž•

The cell state, $C_t$, is the core component of the LSTM, acting as its long-term memory. The update from the previous state $C_{t-1}$ to the current state $C_t$ is the centerpiece of the architecture.

The update equation is:
$$
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
$$

This operation can be broken down into two parts:
1.  **Forget**: The first term, `$f_t * C_{t-1}$`, involves an element-wise multiplication of the old cell state with the output of the forget gate. This selectively removes information deemed no longer relevant.
2.  **Add/Input**: The second term, `$i_t * \tilde{C}_t$`, involves an element-wise multiplication of the new candidate values with the output of the input gate. This selectively adds new information deemed relevant.

The combination of forgetting old information and adding new information results in the updated cell state $C_t$.

<br>
<hr>
<br>

### 3. Mitigating the Vanishing Gradient Problem ðŸ“ˆ

The primary reason LSTMs are effective at learning long-term dependencies lies in the design of the cell state update.

In a simple RNN, the hidden state is updated through a series of nested, multiplicative operations: $h_t = \tanh(W_{hh}h_{t-1} + ...)$. During backpropagation, this leads to repeated multiplication by the weight matrix $W_{hh}$, causing gradients to either vanish or explode.

The LSTM's cell state update, $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$, is fundamentally different. The connection between $C_{t-1}$ and $C_t$ is primarily **additive**. When we compute the gradient during backpropagation, the chain rule gives us:
$$
\frac{\partial C_t}{\partial C_{t-1}} = f_t
$$
The gradient flows from $C_t$ to $C_{t-1}$ by passing through only an element-wise multiplication with the forget gate, $f_t$. There is no multiplication by a shared weight matrix like $W_{hh}$.

This creates an "information superhighway" for the gradient. If the forget gate is set to remember (i.e., its values are close to 1), the gradient can flow backward through many time steps almost unchanged. This additive structure ensures that the error signal can propagate back to much earlier time steps, allowing the network to learn dependencies over long distances in the sequence without the gradient signal dying out.