## Assignment: Perceptron and Loss Function Analysis

### Problem

Prepare a technical report that provides a comprehensive analysis of the foundational concepts of neural networks. Your report must address the following points:

1.  **From Regression to a Single Neuron**: Explain the key limitation of **Linear Regression** that makes it unsuitable for binary classification tasks. How does **Logistic Regression** overcome this limitation? Finally, describe the mathematical relationship between a Logistic Regression model and a single modern neuron (Perceptron).
2.  **Choosing the Right Loss Function**: Describe the purpose of a loss function in the training process. Compare and contrast **Mean Squared Error (MSE)** and **Binary Cross-Entropy**. For each loss function, state the type of problem it is typically used for (regression or classification) and explain *why* it is well-suited for that task.
3.  **Mathematical Analysis of Cross-Entropy**: Using the formula for binary cross-entropy on a single example, $L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$, explain what the loss evaluates to when the true label is $y=1$ and the model's prediction $\hat{y}$ is very good (approaching 1) versus very poor (approaching 0).


<br>
<br>

---

<br>
<br>

## Solution Report: Assignment 4

### Introduction

The Perceptron, the simplest form of a neural network, serves as the fundamental building block for the complex deep learning architectures used today. Understanding its origins in classical statistics and the mechanisms by which it learns is crucial. This report traces the conceptual lineage of a single neuron from linear and logistic regression and provides a detailed analysis of the core loss functions that guide the learning process.

<br>

### 1. The Evolution of a Neuron ðŸ§¬

**a. Linear Regression's Limitation**
Linear regression models the relationship between inputs and a continuous output using a linear equation, $H(x) = W^T x + b$. The primary limitation of this model for classification is that its **output is unbounded**, ranging from $-\infty$ to $+\infty$. For a binary classification task (e.g., yes/no, 1/0), we need an output that can be interpreted as a probability, which must be constrained to the range [0, 1].

**b. Logistic Regression's Solution**
Logistic Regression adapts the linear model for classification by introducing a non-linear "squashing" function. It takes the linear output, $z = W^T x + b$, and passes it through the **sigmoid function**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

This function maps any real-valued input to the range (0, 1). The hypothesis for logistic regression, $H(x) = \sigma(W^T x + b)$, therefore produces an output that can be directly interpreted as the probability of the input belonging to the positive class.

**c. Relationship to a Modern Neuron**
A single modern neuron (or Perceptron) is mathematically **identical to a logistic regression model**. It performs the exact same two-step computation:
1.  It calculates a weighted sum of its inputs and adds a bias ($z = W^T x + b$).
2.  It applies a non-linear activation function to this sum (e.g., the sigmoid function).
Thus, a single neuron is a logistic regression unit, and a multi-layered neural network can be viewed as a structure of interconnected logistic regression units working in concert to model complex, non-linear relationships.

<br>
<hr>
<br>

### 2. Selecting an Appropriate Loss Function ðŸŽ¯

The purpose of a **loss function**, $L(y, \hat{y})$, is to quantify the error or penalty for a single prediction ($\hat{y}$) made by the model compared to the ground truth label ($y$). The goal of training is to find the model parameters (weights and biases) that minimize the average loss over the entire dataset.

**a. Mean Squared Error (MSE)**
* **Task**: **Regression** problems, where the target is a continuous numerical value.
* **Why it's suitable**: The MSE formula, $L = (y - \hat{y})^2$, measures the squared distance between the true value and the predicted value. This is a natural and intuitive way to measure error for numerical quantities. Squaring the error ensures the loss is always positive and penalizes larger errors more significantly than smaller ones.

**b. Binary Cross-Entropy**
* **Task**: **Binary Classification** problems, where the target is one of two classes (0 or 1).
* **Why it's suitable**: Cross-entropy measures the dissimilarity between two probability distributions. In this case, it compares the true distribution (e.g., for a label of '1', the probability of being class '1' is 100%) with the model's predicted probability ($\hat{y}$). It is highly effective because it inflicts a very large penalty for predictions that are confident but wrong (e.g., predicting 0.01 when the true label is 1). This provides a strong gradient signal that helps the model learn quickly and efficiently.

<br>
<hr>
<br>

### 3. Mathematical Analysis of Cross-Entropy

Let's analyze the behavior of the binary cross-entropy loss for a single example when the true label is **$y=1$**.

The formula is:

$$
L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]
$$

Substituting $y=1$, the second term $(1-y)\log(1-\hat{y})$ becomes zero, and the formula simplifies to:

$$
L = -\log(\hat{y})
$$

Now, let's consider two scenarios for the model's prediction, $\hat{y}$:

1.  **Good Prediction ($\hat{y} \to 1$)**: When the model correctly predicts a high probability for the positive class, the loss approaches:

    $$
    L = -\log(1) = 0
    $$

    The loss is minimal, providing little incentive for a large weight update, as the model is already correct.

2.  **Poor Prediction ($\hat{y} \to 0$)**: When the model is confidently wrong, predicting a very low probability for the positive class, the loss approaches:

    $$
    L = -\log(0) \to \infty
    $$
    
    The loss becomes infinitely large, creating a massive penalty and a very steep gradient. This strongly pushes the model to correct its parameters.

This analysis demonstrates that the cross-entropy loss function is perfectly calibrated for classification: it rewards correct, confident predictions with a near-zero loss while severely punishing incorrect, confident predictions.