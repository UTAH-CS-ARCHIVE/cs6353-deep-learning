## Assignment 5: Backpropagation from Scratch

### **Problem 1: The Value of Manual Implementation**

According to the lecture notes, why is it important for a deep learning practitioner to understand how to implement backpropagation manually, even when modern frameworks provide "autograd" functionality? List at least two key reasons.

<br>

### **Problem 2: The Forward Pass**

Before we can backpropagate, we must perform a forward pass. Consider a simple 2-layer MLP with a single data sample. Let the input $X$, the first layer's weights $W_1$, and bias $b_1$ be:

$$
X = \begin{pmatrix} 2 & 3 \end{pmatrix}
$$

$$
W_1 = \begin{pmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{pmatrix}
$$

$$
b_1 = \begin{pmatrix} 0.5 & 0.6 \end{pmatrix}
$$

Calculate the pre-activation output $Z_1 = X \cdot W_1 + b_1$ and the post-activation output $A_1 = \sigma(Z_1)$, where $\sigma$ is the sigmoid function. For simplicity in later steps, let's assume after calculation that:

$$
A_1 = \begin{pmatrix} 0.9 & 0.95 \end{pmatrix}
$$

*(You should still perform the calculation for $Z_1$ and show your work, but use the provided $A_1$ value for subsequent problems).*

<br>

### **Problem 3: Backward Pass - Gradients for the Output Layer**

Continuing from Problem 2, let's assume the forward pass through the second layer has been completed. The activation output from the hidden layer is $A_1 = \begin{pmatrix} 0.9 & 0.95 \end{pmatrix}$, and the initial error gradient flowing back to the output layer is $\delta_2 = \begin{pmatrix} 0.2 & -0.3 \end{pmatrix}$.

Calculate the gradient for the weights of the second layer, $\frac{\partial L}{\partial W_2}$, using the formula:

$$
\frac{\partial L}{\partial W_2} = A_1^T \cdot \delta_2
$$

<br>

### **Problem 4: Backward Pass - Gradients for the Hidden Layer**

This is the final step of backpropagation for our network. We need to propagate the error further back to the first layer. You are given the following:

-   The initial error gradient from the output layer: $\delta_2 = \begin{pmatrix} 0.2 & -0.3 \end{pmatrix}$
-   The weights of the second layer: $W_2 = \begin{pmatrix} 0.5 & 0.6 \\ 0.7 & 0.8 \end{pmatrix}$
-   The input to the first layer: $X = \begin{pmatrix} 2 & 3 \end{pmatrix}$
-   To simplify, assume the calculated error term for the hidden layer is: $\delta_1 = \begin{pmatrix} -0.01 & -0.02 \end{pmatrix}$

Your task is to calculate the gradient for the weights of the first layer, $\frac{\partial L}{\partial W_1}$, using the formula:

$$
\frac{\partial L}{\partial W_1} = X^T \cdot \delta_1
$$

*(Note: The problem provides the final $\delta_1$ value. Conceptually, remember that this $\delta_1$ would have been calculated by propagating $\delta_2$ back through $W_2$ and then multiplying by the derivative of the activation function, i.e., $\delta_1 = (\delta_2 \cdot W_2^T) * \sigma'(Z_1)$).*

<br>
<br>

---

<br>
<br>

## Solution Report: Assignment 5

### **Solution to Problem 1: The Value of Manual Implementation**

Understanding manual backpropagation is crucial for several reasons, even with the convenience of autograd:
1.  **Debugging**: A solid grasp of gradient flow is invaluable for diagnosing and fixing training issues, such as the notorious "vanishing" or "exploding" gradient problems, where a model fails to learn effectively.
2.  **Customization and Research**: If you need to design a novel neural network layer, a custom activation function, or a unique loss function, you will often need to define its backward pass manually. This is impossible without understanding the underlying mechanics.
3.  **Fundamental Knowledge**: It transforms the concept of neural network training from a "black box" into a comprehensible, transparent process, solidifying one's foundational knowledge of the field.

<br>

### **Solution to Problem 2: The Forward Pass**

**Objective:** Calculate $Z_1$ and confirm the dimensions for calculating $A_1$.

**Step 1: Calculate $Z_1 = X \cdot W_1 + b_1$**
The matrix multiplication $X \cdot W_1$ is performed first:

$$
X \cdot W_1 = \begin{pmatrix} 2 & 3 \end{pmatrix} \cdot \begin{pmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{pmatrix}
$$

$$
= \begin{pmatrix} (2 \times 0.1 + 3 \times 0.3) & (2 \times 0.2 + 3 \times 0.4) \end{pmatrix}
$$

$$
= \begin{pmatrix} (0.2 + 0.9) & (0.4 + 1.2) \end{pmatrix} = \begin{pmatrix} 1.1 & 1.6 \end{pmatrix}
$$

Next, add the bias vector $b_1$:

$$
Z_1 = \begin{pmatrix} 1.1 & 1.6 \end{pmatrix} + \begin{pmatrix} 0.5 & 0.6 \end{pmatrix} = \begin{pmatrix} 1.6 & 2.2 \end{pmatrix}
$$

The pre-activation output is $Z_1 = \begin{pmatrix} 1.6 & 2.2 \end{pmatrix}$. Applying the sigmoid function $\sigma(Z_1)$ element-wise would produce the final activation $A_1$.

<br>

### **Solution to Problem 3: Backward Pass - Gradients for the Output Layer**

**Objective:** Calculate the gradient matrix $\frac{\partial L}{\partial W_2}$.

**Given:**
-   $A_1 = \begin{pmatrix} 0.9 & 0.95 \end{pmatrix}$
-   $\delta_2 = \begin{pmatrix} 0.2 & -0.3 \end{pmatrix}$

**Step 1: Transpose the activation vector $A_1$.**
The shape of $A_1$ is $1 \times 2$. Its transpose, $A_1^T$, will have a shape of $2 \times 1$.

$$
A_1^T = \begin{pmatrix} 0.9 \\ 0.95 \end{pmatrix}
$$

**Step 2: Perform the outer product $A_1^T \cdot \delta_2$.**
The result will be a $2 \times 2$ matrix, which matches the dimensions of $W_2$.

$$
\frac{\partial L}{\partial W_2} = \begin{pmatrix} 0.9 \\ 0.95 \end{pmatrix} \cdot \begin{pmatrix} 0.2 & -0.3 \end{pmatrix}
$$

$$
= \begin{pmatrix} 0.9 \times 0.2 & 0.9 \times -0.3 \\ 0.95 \times 0.2 & 0.95 \times -0.3 \end{pmatrix}
$$

$$
= \begin{pmatrix} 0.18 & -0.27 \\ 0.19 & -0.285 \end{pmatrix}
$$

**Final Answer:** The gradient for the second layer's weights is $\frac{\partial L}{\partial W_2} = \begin{pmatrix} 0.18 & -0.27 \\ 0.19 & -0.285 \end{pmatrix}$.

<br>

### **Solution to Problem 4: Backward Pass - Gradients for the Hidden Layer**

**Objective:** Calculate the gradient matrix $\frac{\partial L}{\partial W_1}$.

**Given:**
-   $X = \begin{pmatrix} 2 & 3 \end{pmatrix}$
-   $\delta_1 = \begin{pmatrix} -0.01 & -0.02 \end{pmatrix}$

**Step 1: Transpose the input vector $X$.**
The shape of $X$ is $1 \times 2$. Its transpose, $X^T$, will have a shape of $2 \times 1$.
$$
X^T = \begin{pmatrix} 2 \\ 3 \end{pmatrix}
$$

**Step 2: Perform the outer product $X^T \cdot \delta_1$.**
The result will be a $2 \times 2$ matrix, which matches the dimensions of $W_1$.

$$
\frac{\partial L}{\partial W_1} = \begin{pmatrix} 2 \\ 3 \end{pmatrix} \cdot \begin{pmatrix} -0.01 & -0.02 \end{pmatrix}
$$

$$
= \begin{pmatrix} 2 \times -0.01 & 2 \times -0.02 \\ 3 \times -0.01 & 3 \times -0.02 \end{pmatrix}
$$

$$
= \begin{pmatrix} -0.02 & -0.04 \\ -0.03 & -0.06 \end{pmatrix}
$$

**Final Answer:** The gradient for the first layer's weights is $\frac{\partial L}{\partial W_1} = \begin{pmatrix} -0.02 & -0.04 \\ -0.03 & -0.06 \end{pmatrix}$. With these gradients, we could now update both $W_1$ and $W_2$ using the gradient descent rule.