## Assignment 4: MLP and Backpropagation

### **Problem 1: The XOR Problem and MLPs**

Explain *why* a single-layer perceptron is fundamentally unable to solve the XOR problem. In your answer, use the concept of "linear separability". Then, briefly describe how a Multi-Layer Perceptron (MLP) overcomes this limitation.

<br>

### **Problem 2: The Two Passes of Backpropagation**

Describe the distinct roles of the "forward pass" and the "backward pass" within the backpropagation algorithm. What is the key piece of information that is "propagated" during the backward pass, and what does it represent?

<br>

### **Problem 3: Deconstructing the Gradient Formula**

The lecture provides the core formula for computing the gradient of the loss with respect to the weights of a given layer $l$:

$$
\frac{\partial L}{\partial W^{(l)}} = \delta^{(l+1)} (a^{(l)})^T
$$

Explain the meaning and origin of each of the three main terms in this equation:
a) $\frac{\partial L}{\partial W^{(l)}}$
b) $\delta^{(l+1)}$
c) $(a^{(l)})^T$

<br>

### **Problem 4: Intuition Behind the Gradient**

Using the formula from Problem 3, explain the intuition behind why the update for a weight in layer $l$ would be zero (or close to zero) if the activation of its input neuron (a value in $a^{(l)}$) was zero during the forward pass. Why does this make sense from a "credit assignment" perspective?

<br>
<br>

---

<br>
<br>

## Solution Report: Assignment 4

### **Solution to Problem 1: The XOR Problem and MLPs**

A single-layer perceptron is unable to solve the XOR problem because the XOR function is **not linearly separable**. A single-layer perceptron functions by creating a single linear decision boundary (a straight line in 2D) to separate data points into classes. If you plot the four points of the XOR function, it is impossible to draw a single straight line that separates the '1' outputs from the '0' outputs.

A Multi-Layer Perceptron (MLP) overcomes this by introducing one or more **hidden layers**. These layers allow the network to learn non-linear combinations of the input features. The first layer can learn simple linear boundaries, and the hidden layer can then combine these boundaries to form a more complex, non-linear decision region. For XOR, a hidden layer can learn intermediate functions (like 'A OR B' and 'NOT (A AND B)'), which *are* linearly separable. The output layer then combines these intermediate features to produce the final, non-linear XOR result.

<br>

### **Solution to Problem 2: The Two Passes of Backpropagation**

The two passes in the backpropagation algorithm have distinct and complementary roles:

-   **Forward Pass:** The role of the forward pass is to **make a prediction**. Input data is fed into the network, and it flows forward through the layers, with each neuron computing its weighted sum and activation, until the final output layer produces a prediction, $\hat{y}$. This pass also crucially involves storing the activation values of each layer, as they are needed for the backward pass.

-   **Backward Pass:** The role of the backward pass is to **calculate and propagate the error gradient**. After computing the loss from the forward pass, the algorithm moves backward from the output layer. The key piece of information that is propagated backward is the **error term**, denoted as **$\delta$**. This term represents the gradient of the loss function with respect to the pre-activation input ($z$) of each neuron. This error signal is used via the chain rule to calculate the gradients for the weights at each layer.

<br>

### **Solution to Problem 3: Deconstructing the Gradient Formula**

The formula $\frac{\partial L}{\partial W^{(l)}} = \delta^{(l+1)} (a^{(l)})^T$ is the heart of the weight update calculation in backpropagation.

a) $\frac{\partial L}{\partial W^{(l)}}$: This term is the **gradient of the loss with respect to the weights of layer $l$**. It is a matrix of the same dimensions as the weight matrix $W^{(l)}$, where each element represents how much the total loss $L$ will change for a small change in the corresponding weight. This is the quantity we need to update the weights using an optimizer like Gradient Descent.

b) $\delta^{(l+1)}$: This is the **error term (delta) from the next layer, $l+1$**. It is a vector that represents the gradient of the loss with respect to the pre-activation outputs ($z^{(l+1)}$) of the neurons in layer $l+1$. This term is the error signal that is "propagated backward" from layer $l+1$ to layer $l$ using the chain rule.

c) $(a^{(l)})^T$: This term is the **transpose of the activation output vector from the current layer, $l$**. The vector $a^{(l)}$ contains the outputs of the neurons in layer $l$, which were calculated and stored during the forward pass. These activations served as the inputs to layer $l+1$.

<br>

### **Solution to Problem 4: Intuition Behind the Gradient**

The update for a specific weight is proportional to the product of the error from the output neuron and the activation from the input neuron. The gradient formula $\frac{\partial L}{\partial W^{(l)}} = \delta^{(l+1)} (a^{(l)})^T$ involves an outer product between the error term $\delta^{(l+1)}$ and the activation vector $a^{(l)}$.

If a specific neuron in layer $l$ has an activation of zero (i.e., a value in the vector $a^{(l)}$ is zero), then in the outer product calculation, all the resulting gradient values associated with weights originating from that neuron will also be zero.

The intuition from a **credit assignment** perspective is straightforward and powerful: **if a neuron did not activate (did not "fire"), it could not have contributed to the error in the final prediction.** Therefore, it deserves no "blame" for the error. Consequently, the weights connecting that silent neuron to the next layer should not be changed. The backpropagation algorithm naturally enforces this logic, ensuring that weight updates are only applied where they are relevant.