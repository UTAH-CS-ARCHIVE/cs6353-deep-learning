Good morning, everyone.

Please find the assignment for this week's lecture on advanced CNN architectures below. Following the prompt, I have provided a model report that demonstrates the level of detail and mathematical rigor I expect in your submissions.

<br>
<br>

## Assignment: Mathematical Analysis of Residual Networks (ResNet)

### Prompt

Based on the lecture notes, provide a detailed technical explanation of the ResNet architecture. Your report must address the following points with a clear mathematical focus:

1.  Define the **degradation problem** observed in very deep neural networks and explain why it is a paradoxical optimization challenge.
2.  Formulate the mathematical principle of **residual learning**. Clearly define the functions $H(x)$ and $F(x)$ and their relationship.
3.  Explain how the **skip connection** implements residual learning and how this structure makes learning an identity mapping trivial.
4.  Describe how the ResNet architecture helps mitigate the **vanishing gradient problem** by analyzing the gradient flow during backpropagation.

<br>
<br>

## Report: A Mathematical Analysis of the ResNet Architecture

### 1. Introduction

The development of very deep neural networks has been hindered by significant optimization challenges. The Residual Network (ResNet) architecture, introduced in 2015, provided a groundbreaking solution to a key obstacle known as the degradation problem. This report details the mathematical foundations of ResNet, explaining how its core concept of residual learning and its implementation via skip connections address both network degradation and the vanishing gradient problem, thereby enabling the effective training of networks of unprecedented depth.

<br>

### 2. The Degradation Problem in Deep Networks

The degradation problem refers to the empirical observation that as a network's depth increases, its accuracy gets saturated and then degrades rapidly. Paradoxically, this decline is not caused by overfitting; the **training error** of a deeper model is often higher than that of its shallower counterpart.

Theoretically, a deeper model should produce no higher training error than a shallower one. Consider a shallow network and a deeper network constructed by adding more layers to the shallow one. The deeper model should be able to, at a minimum, learn the performance of the shallower model. A potential solution would be for the added layers to learn an **identity mapping**, where their output is simply equal to their input. Let the function learned by these layers be $I(x)$, the identity mapping is defined as:

$$
I(x) = x
$$

In practice, however, optimizers based on stochastic gradient descent find it exceedingly difficult to compel a stack of non-linear layers (e.g., convolution, ReLU) to approximate such an identity function. This optimization difficulty is the root cause of the degradation problem.

<br>

### 3. The Mathematics of Residual Learning

The core innovation of ResNet is to reframe the learning objective of a block of layers. Instead of having a set of layers learn an underlying desired mapping $H(x)$, ResNet tasks them with learning a **residual function** $F(x)$.

The residual function is defined as the difference between the desired mapping and the input to the block:

$$
F(x) = H(x) - x
$$

By rearranging this equation, the original desired mapping can be expressed as the sum of the residual and the input:

$$
H(x) = F(x) + x
$$

This formulation is implemented architecturally using a **"skip connection"** (or shortcut). The input $x$ "skips" the block of layers and is added element-wise to the output of the block, $F(x)$. The block of layers now only needs to learn the residual $F(x)$.

This elegantly solves the identity mapping problem. If the optimal function is an identity mapping (i.e., $H(x) = x$), the optimizer can easily achieve this by driving the weights of the layers in the block towards zero. This makes the output of the block $F(x) \approx 0$, resulting in:

$$
H(x) \approx 0 + x = x
$$

It is mathematically far simpler for an optimizer to push weights to zero than to fit the complex, non-linear transformations required to approximate an identity function.

<br>

### 4. Mitigating the Vanishing Gradient Problem

A profound benefit of the ResNet structure is its effect on gradient flow during backpropagation. The vanishing gradient problem occurs when gradients become exponentially small as they are propagated back to earlier layers, making them learn very slowly or not at all.

Consider the gradient of the loss, $\mathcal{L}$, with respect to the input of a residual block, $x$. Using the chain rule on the forward propagation equation $H(x) = F(x) + x$, we can express the gradient as:

$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial H} \cdot \frac{\partial H}{\partial x} = \frac{\partial \mathcal{L}}{\partial H} \cdot \left( \frac{\partial F(x)}{\partial x} + \frac{\partial x}{\partial x} \right)
$$

This simplifies to:

$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial H} \cdot \left( \frac{\partial F(x)}{\partial x} + 1 \right)
$$

The crucial term here is the `$ + 1 $`. This term is derived from the identity path of the skip connection. It ensures that even if the gradient passing through the weighted layers, $\frac{\partial F(x)}{\partial x}$, becomes vanishingly small, the gradient from a later layer $\frac{\partial \mathcal{L}}{\partial H}$ can still propagate directly back through this additive term. This creates an uninterrupted "superhighway" for the gradient, ensuring that early layers continue to receive a strong learning signal. This property is what makes it feasible to train networks with hundreds or even thousands of layers.

<br>

### 5. Conclusion

The ResNet architecture's introduction of residual learning is a mathematically elegant and powerful solution to the primary obstacles of training very deep neural networks. By reformulating the learning objective and using skip connections, ResNet directly addresses the degradation problem and provides a robust path for gradient flow, effectively mitigating the vanishing gradient issue. This has fundamentally changed the landscape of deep learning, making extreme depth a viable and effective strategy for building more powerful models.