## Assignment 2: Environment and Library Basics

### **Problem 1: Conceptual Understanding**

Based on the lecture notes, please answer the following two questions concisely:

a) Explain the primary difference in purpose between Anaconda/`conda` and Jupyter Notebook in a data science workflow.
b) What are the two main advantages of using a deep learning framework's Tensor over a standard numerical array for building neural networks?

<br>

### **Problem 2: Vector Operations**

Consider the two vectors $\mathbf{a}$ and $\mathbf{b}$:

$$
\mathbf{a} = \begin{pmatrix} 5 \\ 1 \\ -2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 2 \\ -3 \\ 4 \end{pmatrix}
$$

Calculate the following, showing your work for the dot product:
a) The vector sum $\mathbf{a} + \mathbf{b}$
b) The element-wise product (Hadamard product) $\mathbf{a} \odot \mathbf{b}$
c) The dot product $\mathbf{a} \cdot \mathbf{b}$

<br>

### **Problem 3: Matrix Operations**

Consider the two matrices $X$ and $Y$:

$$
X = \begin{pmatrix} 1 & 0 & 2 \\ 3 & -1 & 1 \end{pmatrix}
$$

$$
Y = \begin{pmatrix} 2 & 1 \\ -1 & 0 \\ 4 & 5 \end{pmatrix}
$$

Calculate the matrix product $Z = XY$. Manually show the calculation for the element $Z_{11}$ (the element in the first row, first column of the resulting matrix) using the matrix multiplication formula $C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$.

<br>

### **Problem 4: Theoretical Application**

Explain *why* the matrix multiplication operation, as calculated in Problem 3, is a fundamental operation in neural networks. Relate your answer to the concept of a linear layer transformation and the data structures (vectors and matrices) we have discussed.

<br>
<br>

---

<br>
<br>

## Solution Report: Assignment 2

### **Solution to Problem 1: Conceptual Understanding**

a) **Anaconda/`conda` vs. Jupyter Notebook:**
The primary difference is that **Anaconda/`conda`** is a tool for **environment and package management**. Its purpose is to create isolated spaces for projects, handling the installation of libraries and resolving their dependencies to avoid conflicts. In contrast, **Jupyter Notebook** is a tool for **interactive computing and development**. Its purpose is to provide a live coding environment where one can execute calculations in segments, visualize results, and document the process in a single, shareable document. In short, `conda` sets up the workshop, and Jupyter is the workbench inside it.

b) **Advantages of Tensors:**
The two main advantages of using Tensors for deep learning are:
1.  **Hardware Acceleration:** Tensors are designed to be used with specialized hardware like Graphics Processing Units (GPUs). GPUs can perform the massive parallel computations required for training neural networks far more quickly than a standard CPU, drastically reducing training time.
2.  **Automatic Differentiation:** Tensors are part of a computational graph that tracks every operation performed on them. This allows the deep learning framework to automatically calculate the gradients (derivatives) of a loss function with respect to any parameter in the model. This mechanism is the engine that powers backpropagation and model learning.

<br>

### **Solution to Problem 2: Vector Operations**

**Given:**
Vectors $\mathbf{a} = \begin{pmatrix} 5 \\ 1 \\ -2 \end{pmatrix}$ and $\mathbf{b} = \begin{pmatrix} 2 \\ -3 \\ 4 \end{pmatrix}$.

**a) Vector Sum:**
The sum is calculated by adding the corresponding elements of each vector.

$$
\mathbf{a} + \mathbf{b} = \begin{pmatrix} 5+2 \\ 1+(-3) \\ -2+4 \end{pmatrix} = \begin{pmatrix} 7 \\ -2 \\ 2 \end{pmatrix}
$$

**b) Element-wise (Hadamard) Product:**
The Hadamard product is calculated by multiplying the corresponding elements.

$$
\mathbf{a} \odot \mathbf{b} = \begin{pmatrix} 5 \times 2 \\ 1 \times (-3) \\ -2 \times 4 \end{pmatrix} = \begin{pmatrix} 10 \\ -3 \\ -8 \end{pmatrix}
$$

**c) Dot Product:**
The dot product is the sum of the element-wise products, resulting in a scalar value.

$$
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{3} a_i b_i = (5 \times 2) + (1 \times -3) + (-2 \times 4)
$$

$$
\mathbf{a} \cdot \mathbf{b} = 10 - 3 - 8 = -1
$$

<br>

### **Solution to Problem 3: Matrix Operations**

**Objective:** To calculate the matrix product $Z = XY$.

**Given:**
A $2 \times 3$ matrix $X$ and a $3 \times 2$ matrix $Y$. The resulting matrix $Z$ will have dimensions $2 \times 2$.

$$
X = \begin{pmatrix} 1 & 0 & 2 \\ 3 & -1 & 1 \end{pmatrix}, \quad Y = \begin{pmatrix} 2 & 1 \\ -1 & 0 \\ 4 & 5 \end{pmatrix}
$$

**Manual Calculation for $Z_{11}$:**
The element $Z_{11}$ is the dot product of the first row of $X$ and the first column of $Y$.

$$
Z_{11} = \sum_{k=1}^{3} X_{1k} Y_{k1} = (X_{11} \times Y_{11}) + (X_{12} \times Y_{21}) + (X_{13} \times Y_{31})
$$

$$
Z_{11} = (1 \times 2) + (0 \times -1) + (2 \times 4) = 2 + 0 + 8 = 10
$$

**Full Calculation of Matrix Z:**
- $Z_{12} = (1 \times 1) + (0 \times 0) + (2 \times 5) = 1 + 0 + 10 = 11$
- $Z_{21} = (3 \times 2) + (-1 \times -1) + (1 \times 4) = 6 + 1 + 4 = 11$
- $Z_{22} = (3 \times 1) + (-1 \times 0) + (1 \times 5) = 3 + 0 + 5 = 8$

**Final Answer:**
The resulting matrix $Z$ is:

$$
Z = \begin{pmatrix} 10 & 11 \\ 11 & 8 \end{pmatrix}
$$

<br>

### **Solution to Problem 4: Theoretical Application**

Matrix multiplication is fundamental to neural networks because it represents the core **linear transformation** that occurs within each layer.

In a neural network, a layer's input data can be represented as a matrix $X$ (where each row is a data sample) and the layer's parameters or "weights" are stored in a matrix $W$. The operation $Z = XW$ calculates the weighted sum of the inputs for every neuron in that layer.

Essentially, matrix multiplication maps the input data from one vector space to another. For example, it might transform a 784-dimensional vector representing an image into a 128-dimensional vector in a hidden "feature space." This transformation allows the network to learn increasingly complex and abstract representations of the data as it passes through successive layers. This linear transformation is then typically followed by a non-linear activation function, but the matrix multiplication is the primary computational step where learning parameters (the weights in matrix $W$) are applied to the data.