Good morning, class.

This week, we begin our study of Convolutional Neural Networks, the architecture that has revolutionized computer vision. The following assignment focuses on the core principles and mathematical foundations discussed in the lecture. You will find the solution report immediately after the questions.

<br>

## Assignment 10: Convolutional Neural Networks I - Core Architecture

**Instructions:** Please provide clear, well-reasoned answers to the following questions, incorporating mathematical formulas and concepts where relevant.

<br>

**Question 1: Calculating Layer Parameters and Output Size**

Consider a convolutional layer that receives a color image with dimensions 64x64x3 (Height x Width x Depth) as input. The layer has the following configuration:
-   Number of filters: 16
-   Filter (kernel) size: 3x3
-   Stride: 1
-   Padding: 0

Calculate the following:
1.  The dimensions (Height, Width, Depth) of the output volume (feature map) produced by this layer.
2.  The total number of learnable parameters (weights and biases) in this convolutional layer.

<br>

**Question 2: Convolution vs. Cross-Correlation**

The lecture notes point out that deep learning libraries implement cross-correlation but refer to it as convolution. Explain why the mathematical distinction between true convolution (with a flipped kernel) and cross-correlation (with an unflipped kernel) is irrelevant for the purpose of training a neural network.

<br>

**Question 3: Equivariance vs. Invariance**

The lecture notes distinguish between the "translation equivariance" of a convolutional layer and the "translation invariance" that pooling helps to achieve.
1.  Define both terms in the context of a CNN.
2.  Explain the specific role of the **Max Pooling** operation in helping the network move from equivariance towards invariance.

<br>

**Question 4: The Core Advantages of CNNs over MLPs for Images**

Summarize the two primary advantages of using a convolutional layer instead of a fully-connected (MLP) layer for processing images. Your explanation should be centered around the concepts of **parameter sharing** and the **preservation of spatial hierarchy**.

<br>
<br>

## Solution Report: Assignment 10

Here are the detailed solutions for the assignment on the core architecture of CNNs.

<br>

### **Solution to Question 1: Calculating Layer Parameters and Output Size**

**Problem Statement:** Calculate the output dimensions and total parameters for a given convolutional layer.

**Solution:**

1.  **Output Volume Dimensions:**
    The formula to calculate the output height or width of a feature map is:
    $$
    O = \frac{W - F + 2P}{S} + 1
    $$
    Where:
    -   $W$ is the input volume size (width or height) = 64
    -   $F$ is the filter size = 3
    -   $P$ is the padding = 0
    -   $S$ is the stride = 1

    Plugging in the values for height and width:
    $$
    O_{width} = O_{height} = \frac{64 - 3 + 2(0)}{1} + 1 = 61 + 1 = 62
    $$
    The **depth** of the output volume is always equal to the number of filters used in the convolutional layer. In this case, the depth is 16.

    Therefore, the dimensions of the output volume are **62x62x16**.

2.  **Total Learnable Parameters:**
    The parameters in a convolutional layer consist of the weights within the filters and one bias term for each filter.
    -   **Weights per filter:** Each filter must have the same depth as the input volume. The number of weights is `(filter_height) x (filter_width) x (input_depth)`.
        $$
        \text{Weights per filter} = 3 \times 3 \times 3 = 27
        $$
    -   **Biases per filter:** Each filter has a single bias term.
        $$
        \text{Biases per filter} = 1
        $$
    -   **Total parameters per filter:**
        $$
        \text{Parameters per filter} = 27 (\text{weights}) + 1 (\text{bias}) = 28
        $$
    -   **Total parameters in the layer:** This is the number of parameters per filter multiplied by the number of filters.
        $$
        \text{Total parameters} = 28 \times 16 = 448
        $$
    This layer has a total of **448 learnable parameters**.

<br>

### **Solution to Question 2: Convolution vs. Cross-Correlation**

**Problem Statement:** Explain why the distinction between convolution and cross-correlation is irrelevant in training neural networks.

**Solution:**
The mathematical distinction is that in a true convolution, the kernel is flipped 180 degrees before the sliding dot product is applied, whereas in cross-correlation, it is not. This distinction is irrelevant in the context of training a neural network because the **weights of the kernel are learnable**.

If the optimal feature detector for a task happens to be represented by a weight matrix $K$, a network using cross-correlation will simply learn the weights of $K$ through backpropagation. If the network were to use true convolution instead, it would simply learn the weights of a flipped version of $K$, let's call it $K_{flipped}$. Since the network is capable of learning *any* set of weights that minimizes the loss function, it does not matter whether the operation is defined with a flip or not. The network will learn the appropriate filter weights for the operation being used.

Given this, deep learning frameworks opt to implement the simpler and slightly more computationally efficient cross-correlation operation and refer to it by the more conventional name "convolution."

<br>

### **Solution to Question 3: Equivariance vs. Invariance**

**Problem Statement:** Define translation equivariance and invariance and explain the role of max pooling.

**Solution:**

1.  **Definitions:**
    -   **Translation Equivariance:** This property belongs to the convolutional layers. It means that if the input to the layer is shifted (translated), the output feature map will also be shifted by the same amount. For example, if an image of a cat is shifted 10 pixels to the right, the feature map corresponding to "cat ears" will also be shifted 10 pixels to the right. The representation of the object moves along with the object itself.
    -   **Translation Invariance:** This is a desirable property for the entire network, especially for classification tasks. It means that if the input is shifted slightly, the final output of the network (e.g., the class probability) should not change. The network should recognize that an image contains a cat, regardless of whether the cat is in the center or slightly off to the side.

2.  **Role of Max Pooling:**
    Max pooling plays a crucial role in transforming equivariance into invariance. A max pooling operation takes the maximum value from a local neighborhood (e.g., a 2x2 window). If a feature (represented by a high activation value) shifts slightly but remains within the same pooling window, the output of the max pooling layer for that window will remain exactly the same. This makes the representation robust to small local translations.

    By repeatedly applying pooling, the network progressively reduces the spatial resolution of the feature maps. This process abstracts the feature's information away from its precise pixel location, retaining only its presence within a larger region. This downsampling and summarization are what build up the network's overall invariance to the object's position.

<br>

### **Solution to Question 4: The Core Advantages of CNNs over MLPs for Images**

**Problem Statement:** Summarize the two main advantages of convolutional layers over fully-connected layers for image data.

**Solution:**

1.  **Parameter Sharing:** This is the most significant advantage. In a fully-connected (MLP) layer, every input neuron is connected to every output neuron with a unique weight. In a convolutional layer, a single small filter (containing a shared set of weights) is applied across all spatial locations of the input image. This means that instead of learning millions of weights, the layer only needs to learn the few weights in its filters. This drastically reduces the number of parameters in the network, which in turn:
    -   Makes the network computationally much more efficient and faster to train.
    -   Significantly reduces the model's tendency to overfit, as it has far fewer degrees of freedom.

2.  **Preservation of Spatial Hierarchy:** Convolutional layers operate on local 2D patches of the input image. This design inherently respects the spatial structure of pixels, acknowledging that nearby pixels are highly related. This allows the network to build a hierarchy of features. Early layers learn to detect simple local patterns like edges and corners. Deeper layers then use the feature maps from earlier layers as input to learn how to combine these simple patterns into more complex and abstract features, such as eyes, noses, or textures. An MLP, by contrast, first flattens the image into a 1D vector, completely destroying this crucial spatial information and treating all pixels as independent inputs.