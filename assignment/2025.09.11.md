## Assignment 5: Weight Initialization Techniques

**Instructions:** Please provide mathematical derivations and clear explanations for each of the following questions.

<br>

**Question 1: The Symmetry Problem**

The lecture notes mention that initializing all weights to zero leads to a "symmetry problem." Consider a single hidden layer in a network. If all weights and biases in this layer are initialized to 0, explain mathematically what happens during the first forward pass and the subsequent first backward pass. Prove that all neurons in the layer will have identical weight updates, and explain why this prevents the network from learning effectively.

<br>

**Question 2: Derivation of Xavier Uniform Initialization**

Glorot/Xavier initialization aims to set the variance of the weights to:

$$
Var(W) = \frac{2}{n_{in} + n_{out}}
$$

When sampling from a uniform distribution $U[-L, L]$, the weights are chosen within a range determined by $L$. The variance of a continuous uniform distribution $U[a, b]$ is given by $Var(X) = \frac{(b-a)^2}{12}$.
Using this information, derive the formula for $L$ for Xavier uniform initialization as presented in the notes:

$$
L = \sqrt{\frac{6}{n_{in} + n_{out}}}
$$

<br>

**Question 3: Mismatch between Xavier Initialization and ReLU Activation**

The lecture notes highlight that He initialization was developed specifically for ReLU. Explain, from a mathematical standpoint of variance propagation, why Xavier initialization is sub-optimal for a network using ReLU activation functions. What happens to the variance of the activations in the forward pass when Xavier is used with ReLU?

<br>

**Question 4: Predicting Experimental Outcomes**

Consider the experiment described in Section 4 of the notes, using a 10-layer MLP with ReLU activations. The notes predict the outcomes for very small, very large, and He initializations.
Predict the outcome if you were to add a fourth experimental condition: **Xavier initialization**.
Describe what the histograms of activations would look like from layer 1 to layer 10. Justify your prediction with the mathematical formula for variance propagation when using Xavier initialization with a ReLU network.

<br>

## Solution Report: Assignment 5

Below are the detailed solutions and explanations for the assignment problems.

<br>

### **Solution to Question 1: The Symmetry Problem**

**Problem Statement:** Explain and prove why zero-initialization leads to a symmetry problem where all neurons learn the same features.

**Solution:**

Let's consider a single hidden layer with $k$ neurons. The pre-activation (input) for neuron $j$ in this layer is $z_j = \mathbf{w}_j^T \mathbf{x} + b_j$.

**1. Forward Pass:**
If all weights and biases are initialized to zero, then for every neuron $j=1, \dots, k$, we have $\mathbf{w}_j = \mathbf{0}$ and $b_j = 0$.
The pre-activation for every neuron is:

$$
z_j = \mathbf{0}^T \mathbf{x} + 0 = 0
$$

The activation for every neuron will be $a_j = f(z_j) = f(0)$, where $f$ is the activation function. Since the input to the activation function is the same (0) for all neurons, their outputs will also be identical:

$$
a_1 = a_2 = \dots = a_k
$$

This means that all neurons in the layer produce the exact same output, regardless of the input $\mathbf{x}$.

**2. Backward Pass:**
During backpropagation, we calculate the gradient of the loss $J$ with respect to the weights. The update for the weight vector $\mathbf{w}_j$ of neuron $j$ is proportional to $\frac{\partial J}{\partial \mathbf{w}_j}$. Using the chain rule:

$$
\frac{\partial J}{\partial \mathbf{w}_j} = \frac{\partial J}{\partial a_j} \frac{\partial a_j}{\partial z_j} \frac{\partial z_j}{\partial \mathbf{w}_j}
$$

-   $\frac{\partial J}{\partial a_j}$: This term represents how much the final loss depends on the output of neuron $j$. Since all $a_j$ are identical and contribute to the next layer in the same way, the gradient passed back to them will also be identical.
-   $\frac{\partial a_j}{\partial z_j}$: This is the derivative of the activation function, evaluated at $z_j=0$. This value, $f'(0)$, is the same for all neurons.
-   $\frac{\partial z_j}{\partial \mathbf{w}_j}$: This is equal to the input vector $\mathbf{x}$.

Combining these, the gradient for the weights of any neuron $j$ is:

$$
\frac{\partial J}{\partial \mathbf{w}_j} = (\text{identical upstream gradient}) \cdot f'(0) \cdot \mathbf{x}
$$

Since all terms on the right-hand side are identical for all neurons $j=1, \dots, k$, their gradients are identical:

$$
\frac{\partial J}{\partial \mathbf{w}_1} = \frac{\partial J}{\partial \mathbf{w}_2} = \dots = \frac{\partial J}{\partial \mathbf{w}_k}
$$

The weight update, $\mathbf{w}_{j, \text{new}} = \mathbf{w}_{j, \text{old}} - \eta \frac{\partial J}{\partial \mathbf{w}_j}$, will be the same for all neurons. Since they started at the same value (0) and receive the same update, they will have identical weights after the first step, and every subsequent step. This symmetry is never broken, meaning all neurons in the layer learn the same feature, making the hidden layer computationally redundant and equivalent to having a single neuron.

<br>

### **Solution to Question 2: Derivation of Xavier Uniform Initialization**

**Problem Statement:** Derive the formula for the bound $L$ for Xavier uniform initialization.

**Solution:**
We are given two facts:
1.  The desired variance for the weights under Xavier initialization is $Var(W) = \frac{2}{n_{in} + n_{out}}$.
2.  The variance of a continuous uniform distribution $U[a, b]$ is $\frac{(b-a)^2}{12}$.

For our specific case, the distribution is $U[-L, L]$, so $a = -L$ and $b = L$. We can calculate the variance for this distribution:

$$
Var(W) = \frac{(L - (-L))^2}{12} = \frac{(2L)^2}{12} = \frac{4L^2}{12} = \frac{L^2}{3}
$$

Now, we set this expression for the variance equal to the target variance from the Xavier formulation:

$$
\frac{L^2}{3} = \frac{2}{n_{in} + n_{out}}
$$

Our goal is to solve for $L$. We can rearrange the equation:

$$
L^2 = 3 \cdot \frac{2}{n_{in} + n_{out}} = \frac{6}{n_{in} + n_{out}}
$$

Taking the square root of both sides gives us the final formula for the initialization bound $L$:

$$
L = \sqrt{\frac{6}{n_{in} + n_{out}}}
$$

This completes the derivation.

<br>

### **Solution to Question 3: Mismatch between Xavier Initialization and ReLU Activation**

**Problem Statement:** Explain mathematically why Xavier initialization is not optimal for ReLU networks.

**Solution:**
Xavier initialization is derived under the assumption that the variance of a layer's output should be equal to the variance of its input. Let $y$ be the pre-activation of a neuron, $y = \sum_{i=1}^{n_{in}} w_i x_i$. The variance is given by:

$$
Var(y) = n_{in} \cdot Var(w_i) \cdot Var(x_i)
$$

Xavier sets the weight variance $Var(w_i)$ to be approximately $\frac{1}{n_{in}}$ (assuming $n_{in} \approx n_{out}$) to satisfy $Var(y) = Var(x_i)$. This works well for linear or symmetric activations like `tanh` where the variance is largely preserved after the activation function is applied.

However, the ReLU activation function, $f(z) = \max(0, z)$, is not symmetric. If we assume the input $z$ to the ReLU function is drawn from a symmetric distribution with mean 0 (like the output of the previous layer), then ReLU will set all negative values (half of the inputs) to zero. This action significantly alters the variance of the distribution.

The variance of the output of a ReLU unit is half the variance of its input:

$$
Var(f(z)) = \frac{1}{2} Var(z)
$$

So, for a layer using ReLU, the variance of the activated outputs becomes:

$$
Var(a) = \frac{1}{2} Var(y) = \frac{1}{2} (n_{in} \cdot Var(w_i) \cdot Var(x_i))
$$

If we use Xavier initialization, where $Var(w_i) \approx \frac{1}{n_{in}}$, the variance of the activations becomes:

$$
Var(a) \approx \frac{1}{2} (n_{in} \cdot \frac{1}{n_{in}} \cdot Var(x_i)) = \frac{1}{2} Var(x_i)
$$

This shows that when using Xavier initialization with ReLU, the variance of the signal is halved at every layer. In a deep network, this repeated halving causes the variance to shrink exponentially towards zero. This leads to vanishing activations and, consequently, vanishing gradients during the backward pass, which severely impedes the training of deep layers.

<br>

### **Solution to Question 4: Predicting Experimental Outcomes**

**Problem Statement:** Predict the histogram shapes for a 10-layer ReLU network using Xavier initialization.

**Solution:**
**Prediction:**
If we run the experiment with Xavier initialization on a 10-layer ReLU network, the histograms of the activations will show a progressive and steady decay in variance from layer 1 to layer 10. Each successive layer's histogram will be narrower than the previous one, with the values clustering more tightly around zero. By the final layers, the activations for the entire batch of data would be very close to zero, visually demonstrating a vanishing signal.

**Mathematical Justification:**
As derived in the solution to Question 3, the relationship between the output variance ($Var(a_l)$) and input variance ($Var(a_{l-1})$) for a layer $l$ using ReLU and Xavier initialization is:

$$
Var(a_l) \approx \frac{1}{2} Var(a_{l-1})
$$

This means that the variance is approximately halved at each layer. For a 10-layer network, the variance of the activations at the 10th layer, $Var(a_{10})$, would be related to the initial input variance $Var(x)$ by:

$$
Var(a_{10}) \approx \left(\frac{1}{2}\right)^{10} Var(x) = \frac{1}{1024} Var(x)
$$

The standard deviation, which dictates the "width" of the histogram, would shrink by a factor of $\sqrt{1/1024} = 1/32$. This dramatic reduction in variance means the signal has effectively vanished by the time it reaches the end of the network. This is precisely why He initialization, which introduces a factor of 2 into the weight variance to counteract ReLU's halving effect, is the appropriate choice.