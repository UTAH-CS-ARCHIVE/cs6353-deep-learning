## Assignment 4: Activation Functions & Gradient Instability

**Instructions:** Please answer the following questions, focusing on providing clear mathematical justifications for your reasoning.

<br>

**Question 1: The Role of Non-Linearity**

The lecture notes state: "Without [activation functions], a multi-layer network would just be a series of linear operations, which is equivalent to a single linear model."
Consider a simple 2-layer neural network (one hidden layer, one output layer) without any activation functions. Let the input be a vector $x$, the weight matrix for the first layer be $W_1$, the bias for the first layer be $b_1$, the weight matrix for the second layer be $W_2$, and the bias for the second layer be $b_2$.
Mathematically prove that the output of this network, $\hat{y}$, is a single linear transformation of the input $x$.

<br>

**Question 2: The Derivative of the Sigmoid Function**

The lecture notes state that the derivative of the sigmoid function, $\sigma(x)$, has a maximum value of 0.25.
First, derive the expression for the derivative of the sigmoid function, $\sigma'(x) = \sigma(x)(1 - \sigma(x))$. Then, prove mathematically that the maximum value of this derivative is indeed 0.25.

<br>

**Question 3: Vanishing Gradients: ReLU vs. Sigmoid**

Using the chain rule as a basis, explain mathematically why the ReLU activation function is more effective at mitigating the vanishing gradient problem than the Sigmoid function in deep networks. Consider a chain of $L$ layers. Let the gradient of the loss $J$ with respect to the output of layer $l$ be $\frac{\partial J}{\partial a^{(l)}}$. The gradient with respect to the output of the previous layer, $a^{(l-1)}$, involves the term $\frac{\partial a^{(l)}}{\partial a^{(l-1)}}$. Expand on this relationship to illustrate the problem.

<br>

**Question 4: The Mathematics of a "Dying ReLU"**

Explain the "Dying ReLU" problem from a mathematical perspective.
Let $a = \max(0, z)$ be the output of a ReLU neuron, where $z = \mathbf{w}^T \mathbf{x} + b$. The weight update rule is given by:

$$
\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \frac{\partial J}{\partial \mathbf{w}}
$$

where $J$ is the loss function and $\eta$ is the learning rate. Show what happens to the partial derivative $\frac{\partial J}{\partial \mathbf{w}}$ when a neuron "dies" (i.e., when its input $z$ is consistently negative), and explain why this stalls the learning for that neuron.

<br>
<br>

## Solution Report: Assignment 4

Here are the detailed solutions for the assignment.

<br>

### **Solution to Question 1: The Role of Non-Linearity**

**Problem Statement:** Prove that a 2-layer network without activation functions is equivalent to a single linear transformation.

**Solution:**

Let the input vector be $x$. The output of the first (hidden) layer, let's call it $h$, is a linear transformation of the input:

$$
h = W_1 x + b_1
$$

Since there is no activation function, this output $h$ is passed directly to the second (output) layer. The final output of the network, $\hat{y}$, is a linear transformation of $h$:

$$
\hat{y} = W_2 h + b_2
$$

We can substitute the expression for $h$ into the equation for $\hat{y}$:

$$
\hat{y} = W_2 (W_1 x + b_1) + b_2
$$

Distributing $W_2$, we get:

$$
\hat{y} = (W_2 W_1) x + (W_2 b_1 + b_2)
$$

Let's define a new weight matrix $W' = W_2 W_1$ and a new bias vector $b' = W_2 b_1 + b_2$. The equation then simplifies to:

$$
\hat{y} = W' x + b'
$$

This final expression is a single linear transformation of the input $x$. This demonstrates that stacking multiple linear layers without a non-linear activation function between them is mathematically equivalent to a single linear layer, thus limiting the network's capacity to learn complex, non-linear patterns.

<br>

### **Solution to Question 2: The Derivative of the Sigmoid Function**

**Problem Statement:** Derive the sigmoid derivative and prove its maximum value is 0.25.

**Solution:**

**Part 1: Derivation**
The sigmoid function is defined as:

$$
\sigma(x) = \frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}
$$

We find its derivative using the chain rule. Let $u = 1 + e^{-x}$, so $\frac{du}{dx} = -e^{-x}$. Then $\sigma(x) = u^{-1}$.

$$
\frac{d\sigma}{dx} = \frac{d\sigma}{du} \frac{du}{dx} = (-1)u^{-2}(-e^{-x}) = \frac{e^{-x}}{(1 + e^{-x})^2}
$$

To get the desired form, we can rewrite this as:

$$
\frac{d\sigma}{dx} = \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}}
$$

We know $\sigma(x) = \frac{1}{1 + e^{-x}}$. We can also write:

$$
1 - \sigma(x) = 1 - \frac{1}{1 + e^{-x}} = \frac{1 + e^{-x} - 1}{1 + e^{-x}} = \frac{e^{-x}}{1 + e^{-x}}
$$

Substituting these back into the derivative expression, we get:

$$
\frac{d\sigma}{dx} = \sigma(x)(1 - \sigma(x))
$$

**Part 2: Finding the Maximum Value**
To find the maximum value of $f(x) = \sigma(x)(1 - \sigma(x))$, we can take its derivative with respect to $x$ and set it to zero. Let $f'(x) = \frac{d}{dx}(\sigma(x) - \sigma(x)^2)$.

$$
f'(x) = \sigma'(x) - 2\sigma(x)\sigma'(x) = \sigma'(x)(1 - 2\sigma(x))
$$

We set $f'(x) = 0$. This occurs when $\sigma'(x) = 0$ or $(1 - 2\sigma(x)) = 0$.
The term $\sigma'(x)$ is always positive, so we must have:

$$
1 - 2\sigma(x) = 0 \implies \sigma(x) = \frac{1}{2}
$$

We find the value of $x$ for which $\sigma(x) = 0.5$:

$$
\frac{1}{1 + e^{-x}} = \frac{1}{2} \implies 1 + e^{-x} = 2 \implies e^{-x} = 1 \implies x = 0
$$

The maximum of the derivative occurs at $x=0$. We now evaluate the derivative at this point:

$$
\sigma'(0) = \sigma(0)(1 - \sigma(0)) = (0.5)(1 - 0.5) = 0.25
$$

Thus, the maximum value of the sigmoid function's derivative is 0.25.

<br>

### **Solution to Question 3: Vanishing Gradients: ReLU vs. Sigmoid**

**Problem Statement:** Explain mathematically why ReLU is more effective than Sigmoid at mitigating vanishing gradients.

**Solution:**

During backpropagation, the gradient of the loss function $J$ with respect to the weights of an early layer is calculated using the chain rule. Consider the gradient of the loss with respect to the activation $a^{(l-1)}$ in layer $l-1$:

$$
\frac{\partial J}{\partial a^{(l-1)}} = \frac{\partial J}{\partial a^{(L)}} \frac{\partial a^{(L)}}{\partial a^{(L-1)}} \cdots \frac{\partial a^{(l)}}{\partial a^{(l-1)}}
$$

where each term can be expanded as:

$$
\frac{\partial a^{(l)}}{\partial a^{(l-1)}} = \frac{\partial a^{(l)}}{\partial z^{(l)}} \frac{\partial z^{(l)}}{\partial a^{(l-1)}} = f'(z^{(l)}) W^{(l)}
$$

Here, $z^{(l)}$ is the pre-activation of layer $l$, $f'$ is the derivative of the activation function, and $W^{(l)}$ is the weight matrix of layer $l$.

The gradient signal propagating backward from the output layer to layer $l-1$ is a product of $L-(l-1)$ such terms. Let's focus on the activation derivatives, $f'(z)$.

**For the Sigmoid function:**
As proven in Question 2, the maximum value of the derivative $\sigma'(z)$ is 0.25. In the saturated regions (where $|z|$ is large), the derivative is close to 0. During backpropagation in a deep network, we are multiplying a long chain of these derivative terms. The gradient for an early layer will contain a product of the form:

$$
\prod_{k=l}^{L} \sigma'(z^{(k)})
$$

Since each $\sigma'(z^{(k)}) \in (0, 0.25]$, this product will shrink exponentially towards zero as the number of layers ($L-l$) increases. For example, $(0.25)^{10} \approx 10^{-6}$. This is the mathematical cause of the **vanishing gradient** problem. The error signal diminishes and becomes too small to update the weights in the early layers effectively.

**For the ReLU function:**
The derivative of the ReLU function, $f(z) = \max(0, z)$, is:

$$
f'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}
$$

For any neuron that is active ($z > 0$), its derivative is 1. The product of derivatives during backpropagation becomes:

$$
\prod_{k=l}^{L} f'(z^{(k)})
$$

If a significant portion of the neurons in the path are active, this product involves many multiplications by 1. Multiplying by 1 does not diminish the magnitude of the gradient. This allows the error signal to propagate back through many layers without vanishing, enabling the training of much deeper networks. While the gradient can still vanish if a neuron is inactive ($f'(z)=0$), it does not suffer from the systematic, guaranteed decay seen with Sigmoid.

<br>

### **Solution to Question 4: The Mathematics of a "Dying ReLU"**

**Problem Statement:** Explain the "Dying ReLU" problem mathematically by analyzing the weight update rule.

**Solution:**

A ReLU neuron is considered "dead" if it consistently receives a negative pre-activation value, $z = \mathbf{w}^T \mathbf{x} + b \le 0$, for all inputs $\mathbf{x}$ in the training data (or a mini-batch).

The output of this neuron is $a = \max(0, z) = 0$.

The weight update rule for a weight vector $\mathbf{w}$ is $\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \frac{\partial J}{\partial \mathbf{w}}$. Let's analyze the gradient term $\frac{\partial J}{\partial \mathbf{w}}$. Using the chain rule, we can write the partial derivative of the loss $J$ with respect to a single weight $w_i$ as:

$$
\frac{\partial J}{\partial w_i} = \frac{\partial J}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial w_i}
$$

Let's analyze each component:
1.  $\frac{\partial J}{\partial a}$: This is the gradient of the loss with respect to the neuron's output, which is passed down from the subsequent layer. It is not necessarily zero.
2.  $\frac{\partial z}{\partial w_i}$: Since $z = \sum_j w_j x_j + b$, we have $\frac{\partial z}{\partial w_i} = x_i$. This is the input corresponding to the weight $w_i$.
3.  $\frac{\partial a}{\partial z}$: This is the derivative of the ReLU activation function. For a "dying" neuron, the input $z$ is always non-positive ($z \le 0$). According to the definition of the ReLU derivative:

    $$
    \frac{\partial a}{\partial z} = f'(z) = 0 \quad \text{for } z \le 0
    $$

Substituting this back into the chain rule expression for the gradient:

$$
\frac{\partial J}{\partial w_i} = \frac{\partial J}{\partial a} \cdot (0) \cdot x_i = 0
$$

Since the partial derivative for every weight $w_i$ in the neuron's weight vector $\mathbf{w}$ is zero, the entire gradient vector is zero:

$$
\frac{\partial J}{\partial \mathbf{w}} = \mathbf{0}
$$

Now, looking at the weight update rule:

$$
\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \cdot \mathbf{0} = \mathbf{w}_{\text{old}}
$$

This shows that the weights of the neuron are never updated. The neuron is stuck in an inactive state and can no longer participate in the learning process, effectively "dying".