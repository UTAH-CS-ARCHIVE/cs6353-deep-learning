# üß† CNN, RNN, and LSTM: Basic Concepts and Mathematical Foundations


## üìå Convolutional Neural Networks (CNN)

CNNs are primarily designed for **grid-structured data** (e.g., images).  
They leverage convolution operations to capture **local spatial patterns**.  

**Key equation (1D convolution):**

$$
h_{i} = \sigma\left(\sum_{j=0}^{k-1} w_{j} \cdot x_{i+j} + b \right)
$$

- $x$: input  
- $w$: kernel/filters  
- $b$: bias  
- $\sigma$: activation function (e.g., ReLU, sigmoid)  

CNNs excel in tasks like **image classification, object detection, and NLP with word embeddings**.



## üîÑ Recurrent Neural Networks (RNN)

RNNs are designed for **sequential data** (e.g., text, speech, time-series).  
They maintain a **hidden state** that propagates through time steps, allowing the model to capture temporal dependencies.  

**Key equation:**

$$
h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
$$

- $x_t$: input at time $t$  
- $h_{t-1}$: hidden state from previous step  
- $W_{xh}, W_{hh}$: weight matrices  
- $b_h$: bias  

The main limitation is **vanishing/exploding gradients**, which makes it hard to capture long-term dependencies.



## ‚è≥ Long Short-Term Memory (LSTM)

LSTMs were introduced to overcome the limitations of vanilla RNNs by adding **gates** that control the flow of information.  

**Key equations:**

- Forget gate:  
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

- Input gate:  
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$  

- Candidate cell state:  
$$\[
\tilde{C}_t = \tanh \left( W_C 
\begin{bmatrix} 
h_{t-1} \\ 
x_t 
\end{bmatrix} 
+ b_C \right)
\]$$

- Cell state update:  
$$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$  

- Output gate:  
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$  

- Hidden state:  
$$h_t = o_t * \tanh(C_t)$$  

These gating mechanisms allow LSTMs to **retain long-term dependencies** effectively.



## üìò Assignment: Analysis of "Quantum-Enhanced Kernel Methods for Stochastic Differential Equations" (arXiv:2508.12861)


### üîé Overview
This paper explores how **quantum kernel methods** can be applied to **Stochastic Differential Equations (SDEs)**, providing both a theoretical foundation and practical methodology. The authors introduce a framework where **Reproducing Kernel Hilbert Spaces (RKHS)** and **quantum feature maps** are leveraged to solve SDE-related learning problems more efficiently.



### üìê Stochastic Differential Equations (SDEs)

A classical SDE has the form:

$$
d x_{t} = f(x_{t},t)\, dt + g(x_{t},t)\, dW_{t}
$$

- **Drift term**: \( f(x_t, t)\, dt \) ‚Üí deterministic part  
- **Diffusion term**: \( g(x_t, t)\, dW_t \) ‚Üí stochastic part driven by Wiener process  
- **Wiener process** \( W_t \) models Brownian motion  

Integral formulation:

$$
X(t) = x_0 + \int_0^t f(X(s), s)\, ds + \int_0^t g(X(s), s)\, dW(s)
$$

The paper re-expresses SDEs as:

$$
X(t) = x_0 + \int_0^t b(X(s))\, ds + \int_0^t \sigma(X(s))\, dW(s)
$$

where \( b \) is drift and \( \sigma \) is diffusion.


### üß© RKHS (Reproducing Kernel Hilbert Space)

RKHS is central to the methodology.  
Given a kernel \( k(x,y) \), RKHS provides a function space with inner product properties:

$$
f(x) = \langle f, k(\cdot, x) \rangle_{\mathcal{H}}
$$

- **Reproducing property**: function values can be represented as inner products  
- **Representer theorem**: solutions to regularized optimization problems take the form

$$
f^{*}(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

Thus, infinite-dimensional optimization is reduced to finite-dimensional.


### ‚öõÔ∏è Quantum Kernels

The authors introduce **quantum feature maps** \( \phi(x) \) to define kernels:

$$
k(x,y) = |\langle \phi(x) | \phi(y) \rangle|^2
$$

Key points:
- Quantum circuits generate embeddings into Hilbert space.  
- Provides richer feature representations than classical kernels.  
- Used to approximate solutions of SDE-related learning tasks.  


### üìù Problem Formulation in the Paper

The target is learning a functional related to SDE solutions:

$$
\mathbb{E}\big[ \ell(X(t)) \big]
$$

where \( \ell \) is some loss functional.  

They formulate the estimation problem in RKHS with a regularization term:

$$
\min_{f \in \mathcal{H}} \sum_{i=1}^n \big( y_i - f(x_i) \big)^2 + \lambda \|f\|_{\mathcal{H}}^2
$$

Using the Representer theorem:

$$
f^{*}(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

Then quantum kernels replace classical \( k \).


### üî¨ Contributions of the Paper

1. **New Framework**: Quantum kernels for approximating functionals of SDEs.  
2. **Generalization Bounds**: Theoretical guarantees on learning efficiency.  
3. **Numerical Examples**: Show that quantum kernels outperform standard kernels on selected SDE tasks.  


### üìä Example: Linear SDE

For a linear drift-diffusion system:

$$
dX_t = aX_t dt + b dW_t
$$

The solution is:

$$
X_t = X_0 e^{at} + b \int_0^t e^{a(t-s)} dW_s
$$

The paper demonstrates how RKHS with quantum kernels can approximate expected values of such processes more efficiently.


### ‚úÖ Conclusion

- **Quantum kernel methods** provide a powerful extension of RKHS techniques for SDE learning problems.  
- They allow for **finite-dimensional, computationally tractable optimization** of problems that originate in infinite-dimensional function spaces.  
- Potential applications include physics-informed machine learning, finance, and stochastic modeling.  
