## Assignment 1: Mathematical Foundations

### **Problem 1: Linear Algebra - Neural Network Layer Transformation**

Consider a single layer in a neural network that takes a 3-dimensional input vector and produces a 2-dimensional output vector. The input vector is given by $x$:

$$
x = \begin{pmatrix} 2 \\ -1 \\ 3 \end{pmatrix}
$$

The weights of the layer are represented by the matrix $W$:

$$
W = \begin{pmatrix} 0.5 & 1.0 & -0.2 \\ -1.5 & 0.1 & 2.0 \end{pmatrix}
$$

And the bias vector is given by $b$:

$$
b = \begin{pmatrix} 0.5 \\ -1.0 \end{pmatrix}
$$

The transformation is defined by the equation $z = Wx + b$. Calculate the output vector $z$.

<br>

### **Problem 2: Linear Algebra - Eigenvectors and Eigenvalues**

Given the square matrix $A$:

$$
A = \begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix}
$$

Verify if the vector $v = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$ is an eigenvector of matrix $A$. If it is, find the corresponding eigenvalue $\lambda$.

<br>

### **Problem 3: Calculus - Gradient Calculation**

Consider a simple loss function $L$ for a model with two parameters, $w_1$ and $w_2$. The function is defined as the squared error:

$$
L(w_1, w_2) = (w_1x_1 + w_2x_2 - y_{true})^2
$$

Given a single data point with inputs $x_1=2$, $x_2=3$ and a true label $y_{true}=7$. Calculate the gradient of the loss function, $\nabla L$, at the point where the weights are $w_1=1$ and $w_2=1$. Explain what the resulting gradient vector signifies in the context of gradient descent.

<br>

### **Problem 4: Probability - Bayes' Theorem**

Imagine you are using a spam filter. Let $S$ be the event that an email is spam, and $W$ be the event that the email contains the word "winner". From historical data, you know the following probabilities:

- The probability that an email is spam: $P(S) = 0.2$
- The probability that a spam email contains the word "winner": $P(W|S) = 0.8$
- The probability that a non-spam email (ham) contains the word "winner": $P(W|S^c) = 0.01$ (where $S^c$ is the complement of $S$).

An email arrives that contains the word "winner". What is the probability that this email is actually spam? That is, find $P(S|W)$.

<br>
<br>

---

<br>
<br>

## Solution Report: Assignment 1

### **Solution to Problem 1: Linear Algebra - Neural Network Layer Transformation**

**Objective:** To calculate the output $z$ of a neural network layer given by $z = Wx + b$.

**Given:**
- Input vector $x = \begin{pmatrix} 2 \\ -1 \\ 3 \end{pmatrix}$
- Weight matrix $W = \begin{pmatrix} 0.5 & 1.0 & -0.2 \\ -1.5 & 0.1 & 2.0 \end{pmatrix}$
- Bias vector $b = \begin{pmatrix} 0.5 \\ -1.0 \end{pmatrix}$

**Step 1: Calculate the matrix-vector product $Wx$.**

The product $Wx$ is computed as follows:

$$
Wx = \begin{pmatrix} 0.5 & 1.0 & -0.2 \\ -1.5 & 0.1 & 2.0 \end{pmatrix} \begin{pmatrix} 2 \\ -1 \\ 3 \end{pmatrix}
$$

$$
Wx = \begin{pmatrix} (0.5 \times 2) + (1.0 \times -1) + (-0.2 \times 3) \\ (-1.5 \times 2) + (0.1 \times -1) + (2.0 \times 3) \end{pmatrix}
$$

$$
Wx = \begin{pmatrix} 1 - 1 - 0.6 \\ -3 - 0.1 + 6 \end{pmatrix} = \begin{pmatrix} -0.6 \\ 2.9 \end{pmatrix}
$$

**Step 2: Add the bias vector $b$.**

Now, we add the result from Step 1 to the bias vector $b$:

$$
z = Wx + b = \begin{pmatrix} -0.6 \\ 2.9 \end{pmatrix} + \begin{pmatrix} 0.5 \\ -1.0 \end{pmatrix}
$$

$$
z = \begin{pmatrix} -0.6 + 0.5 \\ 2.9 - 1.0 \end{pmatrix} = \begin{pmatrix} -0.1 \\ 1.9 \end{pmatrix}
$$

**Final Answer:** The output vector of the layer is $z = \begin{pmatrix} -0.1 \\ 1.9 \end{pmatrix}$.

<br>

### **Solution to Problem 2: Linear Algebra - Eigenvectors and Eigenvalues**

**Objective:** To verify if $v$ is an eigenvector of $A$ and find the corresponding eigenvalue $\lambda$.

A vector $v$ is an eigenvector of a matrix $A$ if it satisfies the equation $Av = \lambda v$, where $\lambda$ is a scalar (the eigenvalue).

**Given:**
- Matrix $A = \begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix}$
- Vector $v = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$

**Step 1: Compute the product $Av$.**

$$
Av = \begin{pmatrix} 4 & -2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} (4 \times 2) + (-2 \times 1) \\ (1 \times 2) + (1 \times 1) \end{pmatrix}
$$

$$
Av = \begin{pmatrix} 8 - 2 \\ 2 + 1 \end{pmatrix} = \begin{pmatrix} 6 \\ 3 \end{pmatrix}
$$

**Step 2: Check if the result $Av$ is a scaled version of $v$.**

We need to see if there is a scalar $\lambda$ such that $Av = \lambda v$.

From Step 1, we have $Av = \begin{pmatrix} 6 \\ 3 \end{pmatrix}$.
The original vector is $v = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$.

We can see that:

$$
\begin{pmatrix} 6 \\ 3 \end{pmatrix} = 3 \times \begin{pmatrix} 2 \\ 1 \end{pmatrix}
$$

This means $Av = 3v$.

**Final Answer:** Yes, the vector $v = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$ is an eigenvector of matrix $A$. The relationship $Av = \lambda v$ holds with the corresponding eigenvalue $\lambda = 3$.

<br>

### **Solution to Problem 3: Calculus - Gradient Calculation**

**Objective:** To calculate the gradient $\nabla L$ of the loss function $L(w_1, w_2)$ at a specific point.

**Given:**
- Loss Function: $L(w_1, w_2) = (w_1x_1 + w_2x_2 - y_{true})^2$
- Data Point: $x_1=2, x_2=3, y_{true}=7$
- Point of Evaluation: $(w_1, w_2) = (1, 1)$

**Step 1: Find the partial derivatives of $L$ with respect to $w_1$ and $w_2$.**

We will use the chain rule. Let $u = w_1x_1 + w_2x_2 - y_{true}$, so $L = u^2$.
The derivative of $L$ with respect to $u$ is $\frac{dL}{du} = 2u$.

Partial derivative with respect to $w_1$:
$$
\frac{\partial L}{\partial w_1} = \frac{dL}{du} \cdot \frac{\partial u}{\partial w_1} = 2(w_1x_1 + w_2x_2 - y_{true}) \cdot (x_1)
$$

Partial derivative with respect to $w_2$:
$$
\frac{\partial L}{\partial w_2} = \frac{dL}{du} \cdot \frac{\partial u}{\partial w_2} = 2(w_1x_1 + w_2x_2 - y_{true}) \cdot (x_2)
$$

**Step 2: Evaluate the partial derivatives at the given point.**

Substitute $x_1=2, x_2=3, y_{true}=7$ and $w_1=1, w_2=1$ into the expressions from Step 1.

First, let's calculate the value of the term inside the parentheses:
$$
w_1x_1 + w_2x_2 - y_{true} = (1)(2) + (1)(3) - 7 = 2 + 3 - 7 = -2
$$

Now, evaluate the partial derivatives:
$$
\frac{\partial L}{\partial w_1} = 2(-2) \cdot (2) = -8
$$

$$
\frac{\partial L}{\partial w_2} = 2(-2) \cdot (3) = -12
$$

**Step 3: Form the gradient vector.**

The gradient $\nabla L$ is the vector of its partial derivatives:
$$
\nabla L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2} \right)
$$

**Final Answer:** The gradient at $(w_1, w_2) = (1, 1)$ is $\nabla L = (-8, -12)$.

**Interpretation:** The gradient vector $\nabla L = (-8, -12)$ points in the direction of the steepest ascent of the loss function at the point $(1,1)$. In gradient descent, we want to *minimize* the loss. Therefore, we would update the weights by moving in the direction *opposite* to the gradient, i.e., in the direction of $(8, 12)$. This ensures we are taking a step towards a lower value of the loss function.

<br>

### **Solution to Problem 4: Probability - Bayes' Theorem**

**Objective:** To calculate the posterior probability $P(S|W)$ using Bayes' Theorem.

Bayes' Theorem is given by:
$$
P(S|W) = \frac{P(W|S)P(S)}{P(W)}
$$

**Given:**
- Prior probability of spam: $P(S) = 0.2$
- Likelihood of "winner" given spam: $P(W|S) = 0.8$
- Likelihood of "winner" given not-spam: $P(W|S^c) = 0.01$

**Step 1: Calculate the prior probability of not-spam, $P(S^c)$.**
$$
P(S^c) = 1 - P(S) = 1 - 0.2 = 0.8
$$

**Step 2: Calculate the total probability of observing the word "winner", $P(W)$.**
We use the law of total probability:
$$
P(W) = P(W|S)P(S) + P(W|S^c)P(S^c)
$$
$$
P(W) = (0.8)(0.2) + (0.01)(0.8)
$$
$$
P(W) = 0.16 + 0.008 = 0.168
$$
This is the "evidence" term in Bayes' theorem.

**Step 3: Apply Bayes' Theorem to find $P(S|W)$.**
Now we have all the components needed:
$$
P(S|W) = \frac{P(W|S)P(S)}{P(W)} = \frac{(0.8)(0.2)}{0.168}
$$

$$
P(S|W) = \frac{0.16}{0.168} \approx 0.95238
$$

**Final Answer:** The probability that an email is spam, given that it contains the word "winner", is approximately **95.24%**. This shows how observing evidence (the word "winner") can dramatically update our belief about whether an email is spam (from a prior of 20% to a posterior of over 95%).