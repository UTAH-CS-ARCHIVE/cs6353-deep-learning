## Assignment 3: Neural Network Basics

### **Problem 1: Conceptual - Logistic Regression vs. The Perceptron**

Based on the lecture notes, explain the mathematical similarity between a single neuron using a sigmoid activation function and a logistic regression model. What was the key difference in the activation function used by the original Perceptron, and what was the implication of this difference for training?

<br>

### **Problem 2: Calculation - The Forward Pass of a Neuron**

Consider a single neuron with 3 inputs. The neuron's weights are given by the vector $W$ and its bias is $b$.

$$
W = \begin{pmatrix} 0.5 \\ -1.0 \\ 0.2 \end{pmatrix}, \quad b = 0.1
$$

Given an input vector $x$:

$$
x = \begin{pmatrix} 4 \\ 2 \\ -1 \end{pmatrix}
$$

Calculate the output of the neuron, $\hat{y}$. Recall that the output is determined by the hypothesis function $H(x) = \sigma(W^T x + b)$, where $\sigma(z)$ is the sigmoid function:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

First, calculate the logit $z = W^T x + b$, and then compute the final output $\hat{y}$. You may round your final answer to four decimal places.

<br>

### **Problem 3: Calculation - Mean Squared Error (MSE) Loss**

A regression model is tasked with predicting a continuous value. For three different data points, the model's predictions ($\hat{y}$) and the true values ($y$) are as follows:

-   Sample 1: $y_1=3.0$, $\hat{y}_1=2.5$
-   Sample 2: $y_2=1.5$, $\hat{y}_2=2.0$
-   Sample 3: $y_3=4.0$, $\hat{y}_3=4.2$

Calculate the Mean Squared Error (MSE) for this set of predictions using the formula:
$$
L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

<br>

### **Problem 4: Calculation - Binary Cross-Entropy Loss**

A classification model outputs a probability $\hat{y}$ that an input belongs to the positive class (class 1). For two separate examples, calculate the binary cross-entropy loss. Recall the formula:

$$
L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]
$$

*(Note: We use the natural logarithm, denoted as $\log$ or $\ln$.)*

a) **Confident and Correct:** The true label is $y=1$ and the model predicts $\hat{y}=0.9$.
b) **Confident and Incorrect:** The true label is $y=1$ and the model predicts $\hat{y}=0.1$.

Compare the two loss values. What does this tell you about how the cross-entropy loss function penalizes incorrect predictions?

<br>
<br>

---

<br>
<br>

## Solution Report: Assignment 3

### **Solution to Problem 1: Conceptual - Logistic Regression vs. The Perceptron**

A single neuron with a sigmoid activation function is mathematically identical to a logistic regression model. Both models compute a linear combination of the inputs and weights, add a bias, and then pass this result through a sigmoid function. The hypothesis for both is precisely $H(x) = \sigma(W^T x + b)$, which produces an output between 0 and 1 that can be interpreted as a probability.

The key difference in the **original Perceptron** was its use of a **step function** for activation, defined as $\text{step}(z) = 1$ if $z > \theta$ and $0$ otherwise. The critical implication of using a step function is that it is **non-differentiable**; its derivative is zero everywhere except at the threshold, where it is undefined. This prevents the use of gradient-based optimization methods (like Gradient Descent), which are the foundation of modern neural network training. The smooth, differentiable nature of the sigmoid function, in contrast, allows for the efficient calculation of gradients, which is essential for learning the model's parameters.

<br>

### **Solution to Problem 2: Calculation - The Forward Pass of a Neuron**

**Objective:** To calculate the neuron's output $\hat{y} = \sigma(W^T x + b)$.

**Given:**
- $W = \begin{pmatrix} 0.5 \\ -1.0 \\ 0.2 \end{pmatrix}$, $x = \begin{pmatrix} 4 \\ 2 \\ -1 \end{pmatrix}$, $b = 0.1$

**Step 1: Calculate the logit $z = W^T x + b$.**
First, we compute the transpose of $W$, which is $W^T = \begin{pmatrix} 0.5 & -1.0 & 0.2 \end{pmatrix}$.
Next, we perform the dot product $W^T x$:

$$
W^T x = (0.5 \times 4) + (-1.0 \times 2) + (0.2 \times -1)
$$

$$
W^T x = 2 - 2 - 0.2 = -0.2
$$

Now, we add the bias:

$$
z = W^T x + b = -0.2 + 0.1 = -0.1
$$

**Step 2: Apply the sigmoid function $\sigma(z)$.**
Using the value $z = -0.1$, we plug it into the sigmoid formula:

$$
\hat{y} = \sigma(-0.1) = \frac{1}{1 + e^{-(-0.1)}} = \frac{1}{1 + e^{0.1}}
$$

We know that $e^{0.1} \approx 1.10517$.

$$
\hat{y} = \frac{1}{1 + 1.10517} = \frac{1}{2.10517} \approx 0.4750
$$

**Final Answer:** The logit is $z = -0.1$ and the final output of the neuron is $\hat{y} \approx 0.4750$.

<br>

### **Solution to Problem 3: Calculation - Mean Squared Error (MSE) Loss**

**Objective:** To calculate the MSE for the given predictions.

**Given:**
- $y = [3.0, 1.5, 4.0]$
- $\hat{y} = [2.5, 2.0, 4.2]$
- $N = 3$

**Step 1: Calculate the squared error for each sample.**
- Error 1: $(y_1 - \hat{y}_1)^2 = (3.0 - 2.5)^2 = (0.5)^2 = 0.25$
- Error 2: $(y_2 - \hat{y}_2)^2 = (1.5 - 2.0)^2 = (-0.5)^2 = 0.25$
- Error 3: $(y_3 - \hat{y}_3)^2 = (4.0 - 4.2)^2 = (-0.2)^2 = 0.04$

**Step 2: Calculate the mean of the squared errors.**

$$
L = \frac{1}{3} (0.25 + 0.25 + 0.04) = \frac{1}{3} (0.54) = 0.18
$$

**Final Answer:** The Mean Squared Error for the predictions is $0.18$.

<br>

### **Solution to Problem 4: Calculation - Binary Cross-Entropy Loss**

**Objective:** To calculate the binary cross-entropy loss for two cases.

The loss formula is $L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$.

**a) Confident and Correct:** $y=1, \hat{y}=0.9$
Since $y=1$, the $(1-y)$ term becomes zero, simplifying the formula to $L = -[1 \cdot \log(0.9) + 0 \cdot \log(1-0.9)] = -\log(0.9)$.

$$
L = -\log(0.9) \approx -(-0.1054) \approx 0.1054
$$

The loss is a small positive value, close to zero, reflecting a good prediction.

**b) Confident and Incorrect:** $y=1, \hat{y}=0.1$
Again, since $y=1$, the formula simplifies to $L = -[1 \cdot \log(0.1) + 0 \cdot \log(1-0.1)] = -\log(0.1)$.

$$
L = -\log(0.1) \approx -(-2.3026) \approx 2.3026
$$

The loss is a much larger positive value.

**Comparison and Interpretation:**
The loss for the incorrect prediction ($2.3026$) is significantly higher—more than 20 times larger—than the loss for the correct prediction ($0.1054$). This demonstrates that the **cross-entropy loss function heavily penalizes confident but incorrect predictions**. As the predicted probability $\hat{y}$ moves further away from the true label $y$, the loss increases exponentially, creating a strong gradient that pushes the model's weights to make better predictions during training.