## Lecture 06 Notes: Optimization Techniques for Deep Learning

### **Part 1: Beyond Basic Gradient Descent**

#### **The Limitations of Standard Gradient Descent**

So far, we've established that we train neural networks by minimizing a loss function, and we do this using an algorithm called **Gradient Descent**. The concept is simple and intuitive: we calculate the slope (gradient) of the loss function with respect to our weights and take a small step in the opposite direction. We repeat this until we find a minimum.

However, this basic approach, often called "vanilla" or "batch" gradient descent, has several significant limitations, especially when dealing with the complex loss landscapes of deep neural networks:

1.  **Slow Convergence:** The loss surface can have flat regions or long, narrow ravines. In these ravines (where the surface curves much more steeply in one dimension than another), standard gradient descent tends to oscillate back and forth across the narrow axis while making slow progress along the bottom of the ravine. This can make convergence painfully slow.
    

2.  **Local Minima and Saddle Points:** The loss landscapes of deep networks are highly non-convex, meaning they are filled with numerous local minima (sub-optimal valleys) and saddle points. Standard gradient descent can easily get trapped in a local minimum and never find the global minimum. Saddle points are even more problematic in high dimensions; they are points where the gradient is zero, but it's not a minimum. Gradient descent can get stuck on these plateaus for a long time.

3.  **Computational Inefficiency:** In its purest form, Batch Gradient Descent requires calculating the gradient over the *entire* training dataset before making a single update. For modern datasets with millions of samples, this is computationally infeasible. (Note: In practice, we almost always use Mini-batch Gradient Descent, which calculates the gradient on a small subset, or "batch," of data. This is much more efficient but introduces more noise into the gradient estimates.)

To overcome these challenges, a family of more sophisticated optimization algorithms has been developed.

<br>

---

<br>

### **Part 2: Advanced Optimization Algorithms**

#### **1. Gradient Descent**

This is our starting point. The update rule is the simplest, directly translating the gradient into a step.
The formula is:

$$
W \leftarrow W - \eta \nabla_W L
$$

Here, $W$ represents the weights, $L$ is the loss function, and $\nabla_W L$ is the gradient of the loss with respect to the weights. The hyperparameter $\eta$ is the **learning rate**, which controls the size of each step we take. Choosing a good learning rate is critical: too small, and training is too slow; too large, and we might overshoot the minimum and diverge.

<br>

#### **2. Momentum**

The **Momentum** optimizer was developed to accelerate gradient descent, particularly in overcoming the problem of ravines and saddle points. It introduces a "velocity" term, $v$, which accumulates an exponentially decaying moving average of past gradients.

The core idea is inspired by physics: imagine a ball rolling down the loss landscape. Instead of just following the local slope (the gradient), the ball has **momentum**, causing it to continue in the same direction it was already going. This helps it to smooth out the oscillations in the ravines and power through small local minima or flat plateaus.

The update equations are:

$$
v_t = \gamma v_{t-1} + \eta \nabla_W L
$$

$$
W \leftarrow W - v_t
$$

-   $v_t$ is the velocity vector at time step $t$.
-   $\gamma$ is the **momentum term** (a hyperparameter, typically set to around 0.9). It's the friction coefficient; it determines how much of the previous velocity is retained.
-   The new velocity is a combination of the previous velocity ($ \gamma v_{t-1} $) and the current gradient ($ \eta \nabla_W L $). The weight update is then performed using this velocity vector instead of just the raw gradient.

<br>

#### **3. Adam (Adaptive Moment Estimation)**

**Adam** is currently one of the most popular and effective general-purpose optimizers. It combines the idea of momentum with the concept of **adaptive learning rates**. Instead of using a single global learning rate $\eta$ for all parameters, Adam computes an individual, adaptive learning rate for each parameter.

It achieves this by keeping track of two different moving averages for each parameter:

1.  **The 1st Moment (the mean of the gradients):** This is the momentum part. It's stored in a vector $m_t$.
2.  **The 2nd Moment (the uncentered variance of the gradients):** This is the adaptive part. It's stored in a vector $v_t$.

The update equations are:

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_W L
$$

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_W L)^2
$$

-   $m_t$ and $v_t$ are the estimates of the mean and uncentered variance of the gradients, respectively.
-   $\beta_1$ and $\beta_2$ are hyperparameters that control the decay rates of these moving averages (common default values are $\beta_1 = 0.9$, $\beta_2 = 0.999$). The $(\nabla_W L)^2$ is an element-wise square.

Because $m_t$ and $v_t$ are initialized as zeros, they are biased towards zero, especially during the initial steps. Adam corrects for this bias:

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
$$

$$
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

Finally, the weights are updated using these bias-corrected estimates:

$$
W \leftarrow W - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

The key part is the term $\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$. The update is scaled by the inverse of the square root of the second moment estimate ($v_t$). This means that if a parameter has a large gradient variance (i.e., its gradient has been large recently), it will get a smaller learning rate. Conversely, if a parameter has a small gradient variance, it will get a larger learning rate. This per-parameter adaptation makes Adam very robust and often allows it to converge much faster than other methods. The $\epsilon$ is a very small number (e.g., $10^{-8}$) added for numerical stability to prevent division by zero.