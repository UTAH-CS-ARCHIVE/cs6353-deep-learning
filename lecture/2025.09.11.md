# Lecture 05 Notes: Weight Initialization Techniques

## 1. The Importance of Proper Initialization

The starting values of the weights in a neural network can have a profound impact on the training process. A naive approach might be to initialize all weights to zero, but this leads to a symmetry problem where all neurons in a layer learn the exact same features. A slightly better approach is to use small random numbers, but this is not robust enough for deep networks.

Poor initialization can lead to two major problems during training:

- **Vanishing Gradients**: In a deep network, gradients are calculated via the chain rule, meaning they are multiplied back through each layer. If the weights are too small, this product of many small numbers causes the gradient to shrink exponentially, becoming effectively zero by the time it reaches the early layers. As a result, the early layers of the network fail to learn. This is a classic problem with saturating activation functions like `sigmoid` and `tanh`. 
<br>

- **Exploding Gradients**: Conversely, if the weights are too large, the product of many large numbers causes the gradient to grow exponentially. This leads to massive, unstable updates to the weights, and the training process diverges, often resulting in the loss becoming `NaN` (Not a Number). 

The goal of a good weight initialization strategy is to ensure that the variance of the activations and the variance of the gradients remain roughly constant across all layers. This keeps the signal flowing properly in both the forward and backward passes.

<br>

## 2. Xavier / Glorot Initialization

Proposed by Glorot and Bengio in 2010, this technique was designed to work well with symmetric, saturating activation functions like `tanh` and `sigmoid`.

### Mathematical Rationale
The core idea is to set the initial weights such that the variance of the outputs of a layer is equal to the variance of its inputs.

Consider a single neuron's output (before activation): $y = Wx = \sum_{i=1}^{n_{in}} w_i x_i$.
Assuming the inputs and weights are independent and have a mean of 0, the variance of the output is:

$$
Var(y) = n_{in} \cdot Var(w_i) \cdot Var(x_i)
$$

- **Forward Pass**: To keep the variance of the output equal to the input ($Var(y) = Var(x_i)$), we need to satisfy:

$$
n_{in} \cdot Var(w_i) = 1 \implies Var(w_i) = \frac{1}{n_{in}}
$$

- **Backward Pass**: To keep the variance of the gradients the same when flowing backward, a similar derivation shows we need:

$$
n_{out} \cdot Var(w_i) = 1 \implies Var(w_i) = \frac{1}{n_{out}}
$$

Glorot proposed a compromise between these two constraints, using the average of the number of input ($n_{in}$) and output ($n_{out}$) neurons.

$$
Var(W) = \frac{2}{n_{in} + n_{out}}
$$

### How to Use It
- For a **normal distribution**, initialize weights from $\mathcal{N}(0, \sigma^2)$ where $\sigma^2 = \frac{2}{n_{in} + n_{out}}$.
- For a **uniform distribution**, initialize from $U[-L, L]$ where $L = \sqrt{\frac{6}{n_{in} + n_{out}}}$.

This method works well for `tanh` because it assumes the activations are centered around zero, which is true for `tanh`.

<br>

## 3. He Initialization

Proposed by Kaiming He et al. in 2015, this technique was specifically designed for the **ReLU (Rectified Linear Unit)** activation function and its variants.

### Mathematical Rationale
The ReLU function, $f(x) = max(0, x)$, is not zero-centered. For an input that is centered at zero, ReLU sets all negative values to zero, effectively discarding half of the information. This halves the variance of its output.

The variance equation for the output of a layer using ReLU becomes:

$$
Var(y) = \frac{1}{2} n_{in} \cdot Var(w_i) \cdot Var(x_i)
$$

To maintain the variance of the output equal to the input ($Var(y) = Var(x_i)$) in the forward pass, we must now satisfy:

$$
\frac{1}{2} n_{in} \cdot Var(w_i) = 1 \implies Var(w_i) = \frac{2}{n_{in}}
$$

### How to Use It
- For a **normal distribution**, initialize weights from $\mathcal{N}(0, \sigma^2)$ where $\sigma^2 = \frac{2}{n_{in}}$.

He initialization is the standard and recommended approach for networks that use ReLU or Leaky ReLU activations, as it directly accounts for the change in variance caused by these functions.

<br>

## 4. Experimental Confirmation

The effects of different initialization schemes can be observed experimentally without even fully training the network.

- **The Experiment**:
  1. Construct a deep Multi-Layer Perceptron (e.g., 10 layers) with a chosen activation function (e.g., ReLU).
  2. Initialize the weights using three different strategies:
     a. **Bad (Small)**: A normal distribution with a very small standard deviation, e.g., $\mathcal{N}(0, 0.01^2)$.
     b. **Bad (Large)**: A normal distribution with a large standard deviation, e.g., $\mathcal{N}(0, 1^2)$.
     c. **Correct (He)**: The appropriate He initialization, $\mathcal{N}(0, \sqrt{2/n_{in}})$.
  3. Pass a batch of data through the network once (a single forward pass).
  4. Plot histograms of the activation outputs for each of the 10 layers.

- **Expected Results**:
  - **(a) Bad (Small) Init**: The histograms will show the activations shrinking with each layer. The mean and variance will progressively get closer to zero. This is a direct visualization of the **vanishing gradient** problem at its origin.
  - **(b) Bad (Large) Init**: The histograms will show the activations growing larger with each layer. The variance will explode, and many values will be extremely large. This is the precursor to the **exploding gradient** problem.
  - **(c) Correct (He) Init**: The histograms of the activations for every layer will look roughly similar. The mean and variance will remain stable across the entire depth of the network, indicating a healthy flow of data. This setup is poised for successful training.