## Week 8: Convolutional Neural Networks (CNN) II - Advanced Architectures

### **Part 1: A Tour Through Landmark CNN Architectures**

Last week, we learned about the basic building blocks of CNNs. This week, we'll look at how these blocks have been assembled into progressively more powerful and complex architectures. These models are the "celebrities" of the deep learning world, each representing a significant milestone in the evolution of computer vision.

-   **LeNet-5 (1998):**
    This is the pioneer. Developed by Yann LeCun, LeNet-5 was one of the earliest commercial applications of CNNs, famously used for reading handwritten digits on checks. Its architecture is simple by today's standards but established the fundamental pattern: a sequence of **Convolution -> Pooling -> Convolution -> Pooling** layers, followed by fully connected layers for classification. It was the proof of concept that this architectural style worked.

-   **AlexNet (2012):**
    This is the network that started the deep learning revolution. AlexNet won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a massive margin. Its architecture was deeper and wider than LeNet-5. Key innovations included:
    * Using the **ReLU** activation function, which was much faster to train than Sigmoid/Tanh.
    * Implementing **Dropout** layers to combat overfitting.
    * Leveraging **GPUs** for training, which was critical for handling the increased computational load.
    AlexNet proved that deep CNNs could achieve state-of-the-art results on challenging computer vision tasks.

-   **VGGNet (2014):**
    The VGGNet architecture is all about simplicity and depth. The researchers from Oxford's Visual Geometry Group explored a single question: what happens if we just go deeper? They did this by using an extremely uniform architecture, stacking multiple **3x3 convolutional filters** in a row. A stack of two 3x3 filters has an effective receptive field of a 5x5 filter, but with fewer parameters and more non-linearities. VGGNet showed that depth was a critical component for performance, reaching 16-19 layers. Its main drawback is the very large number of parameters.
    

-   **GoogLeNet / Inception (2014):**
    GoogLeNet, the winner of the 2014 ILSVRC, tackled the problem from a different angle. Instead of just going deeper, it also went "wider" by introducing the **Inception module**. The idea was to design a network that was more computationally efficient. An Inception module performs several different convolution and pooling operations (e.g., 1x1, 3x3, 5x5 convolutions) on the same input in parallel and then concatenates their outputs. This allows the network to learn spatial patterns at multiple scales simultaneously. The clever use of **1x1 convolutions** as "bottlenecks" also helped to dramatically reduce the number of parameters.

-   **ResNet (Residual Network) (2015):**
    ResNet was another massive breakthrough, allowing for the successful training of networks that were previously unimaginably deep (over 150 layers). It addressed a key problem that emerged with VGGNet: simply stacking more layers led to a **degradation** problem, where the training accuracy would get worse, not better. ResNet's solution, which we'll detail below, was so effective it has become a standard component in many modern network designs.

<br>

---

<br>

### **Part 2: The Mathematics of ResNet**

#### **The Degradation Problem**

One would expect that as you add more layers to a network, its performance should only get better or, at worst, stay the same. A deeper model should be able to learn everything a shallower model can, plus more complex features. In the worst-case scenario, the extra layers could just learn to be **identity mappings** (i.e., just pass the input through without changing it), and the performance should be the same as the shallower model.

However, in practice, this is not what happened. Researchers found that as networks got deeper, their training error *increased*. This wasn't due to overfitting (as the *training* error itself was worse), but because the deeper models were genuinely harder to optimize using standard gradient descent. The solvers struggled to learn the identity mappings through the stacked non-linear layers.

<br>

#### **Residual Learning and Skip Connections**

The authors of ResNet proposed a brilliant solution: what if we make it easier for the layers to learn an identity mapping? They did this with **Residual Learning**.

Instead of hoping a stack of layers learns the desired underlying mapping $H(x)$, we reframe the problem. Let the layers learn a **residual function**, $F(x)$, which is the difference between the desired mapping and the input: $F(x) = H(x) - x$.

The original mapping can then be expressed as:
$$
H(x) = F(x) + x
$$

This is implemented in the architecture using a **"skip connection"** or **"shortcut connection"**. The input $x$ to a block of layers is passed around the block and added element-wise to the output of the block, $F(x)$.



How does this solve the degradation problem?
If the optimal function is indeed an identity mapping, it's now trivial for the network to learn. The optimizer can simply drive the weights of the layers in $F(x)$ to zero, causing $F(x)$ to become zero. In that case, $H(x) = 0 + x = x$. It's much easier for a network to push weights to zero than to fit an identity mapping through multiple non-linear layers.

Most importantly, this structure helps with the **vanishing gradient problem**. During backpropagation, the gradient can flow directly through the skip connection ($+x$) path. This path has no non-linearities or weight layers to diminish the gradient. This creates a more direct "superhighway" for the gradient signal to travel back to the earlier layers, making it possible to train networks with hundreds or even thousands of layers.