### **Course Notes: Week 2 (Advanced/Lab Session)**
**Topic: ResNet Paper Review and Architecture Analysis**

***

### **Part 1: Paper Review - "Deep Residual Learning for Image Recognition" (He et al., 2015)**

#### **1.1. The Problem: The Degradation of Deep Networks**

A central question in neural network architecture design has been whether simply stacking more layers (i.e., making the network "deeper") leads to better performance. Intuitively, a deeper model should have a richer feature hierarchy and should not produce a higher training error than its shallower counterpart. A deeper model could, in theory, learn the identity function for the added layers and perform at least as well as the shallower model.

However, the authors of the ResNet paper observed a phenomenon they called **degradation**. This is where very deep "plain" networks (simple stacks of convolutional layers) show higher training and test error than their shallower counterparts. This is not simply due to overfitting, as the *training error* itself increases, indicating that the deeper models are harder to optimize. The paper hypothesizes that optimization algorithms struggle to learn identity mappings using a stack of non-linear layers.

<br>

#### **1.2. The Core Idea: Residual Learning**

To address the degradation problem, the paper introduces the concept of a **residual learning framework**.

Instead of hoping that a stack of layers can learn an underlying mapping, denoted as $H(x)$, the framework explicitly reformulates the layers to learn a *residual mapping*:

$$
F(x) := H(x) - x
$$

The original mapping, $H(x)$, is then recast as $F(x) + x$. The hypothesis is that it is easier to optimize the residual mapping $F(x)$ than the original mapping $H(x)$. In the extreme case, if an identity mapping were optimal, it would be much easier for the solver to push the weights of the non-linear layers towards zero to learn $F(x) = 0$ than to fit an identity mapping $H(x) = x$ by adjusting the weights.

This reformulation is realized through "shortcut connections" (or "skip connections"). These connections bypass one or more layers and perform an identity mapping, adding their input directly to the output of the stacked layers.

The output of a residual block can be formally defined as:

$$
y = F(x, \{W_i\}) + x
$$

Here, $x$ is the input to the block, and $y$ is the output. $F(x, \{W_i\})$ represents the residual mapping to be learned by the layers within the block (e.g., a stack of convolutional layers with weights $W_i$). The operation $F+x$ is performed by element-wise addition.

<br>

#### **1.3. Identity vs. Projection Shortcuts**

A key implementation detail is how to handle the addition when the dimensions of $x$ and $F(x)$ do not match. This typically occurs when a convolutional layer in the block has a stride of 2 (downsampling the spatial dimensions) or when the number of channels increases.

The paper proposes two main strategies:
1.  **Identity Shortcut:** When dimensions match, the shortcut connection simply passes the input through (an identity mapping). When dimensions increase, zero-padding can be used to match the channel depth, and a stride of 1 is used.
2.  **Projection Shortcut:** A projection, typically implemented as a 1x1 convolution, is used in the shortcut connection to match the dimensions (both spatial and channel-wise) of the residual output $F(x)$. This adds extra parameters but provides more representational power.

The experiments in the paper showed that while both approaches were effective at solving the degradation problem, the projection shortcut offered slightly better performance.

<br>

#### **1.4. Key Results and Contributions**

* **Solving Degradation:** ResNet successfully trained networks far deeper than previously possible (e.g., 152 layers) and showed that deeper ResNet models consistently achieved lower error rates than shallower ones on datasets like ImageNet and CIFAR-10.
* **State-of-the-Art Performance:** A ResNet ensemble won 1st place in the ILSVRC 2015 classification task with a top-5 error rate of 3.57%.
* **Improved Gradient Flow:** The shortcut connections create a more direct path for gradients to flow during backpropagation, mitigating the vanishing gradient problem that plagues very deep networks.

***

### **Part 2: Architectural Analysis of the Residual Block**

The core building block of a ResNet is the residual block. The paper explores two main variations.

#### **2.1. Basic Block**

This block is used in shallower architectures like ResNet-18 and ResNet-34. Its structure is:
* `3x3 Convolution`, with Batch Normalization (BN) and ReLU activation.
* `3x3 Convolution`, with BN.
* The input `x` (from the shortcut connection) is added to the output of the second BN layer.
* A final `ReLU` activation is applied to the sum.

This simple design is effective for networks where the channel depth is not excessively large.

<br>

#### **2.2. Bottleneck Block**

For deeper networks like ResNet-50, ResNet-101, and ResNet-152, where the number of channels is much larger, the basic block becomes computationally expensive. The **bottleneck block** was introduced for greater efficiency. Its structure is:
* `1x1 Convolution` to reduce the number of channels (e.g., from 256 to 64). This is the "bottleneck". It is followed by BN and ReLU.
* `3x3 Convolution` on the reduced-dimension feature map (e.g., 64 channels). This is the main spatial filtering step. It is followed by BN and ReLU.
* `1x1 Convolution` to restore or increase the number of channels (e.g., from 64 back to 256). It is followed by BN.
* Similar to the basic block, the shortcut connection `x` is added to the output of the final BN layer, followed by a final ReLU activation.

This design has a similar time complexity to two 3x3 convolutions but can handle a much larger number of input/output channels, making deeper models computationally feasible.

***

### **Part 3: Transfer Learning with Pre-trained ResNet Models**

#### **3.1. Concept of Transfer Learning**

Transfer learning is a technique where a model trained on one large-scale task is repurposed and adapted for a second, related task. Instead of training a new model from scratch, which requires a vast amount of labeled data and computational power, we leverage the "knowledge" (i.e., the learned weights and features) from a pre-trained model.

ResNet models trained on the ImageNet dataset are excellent candidates for transfer learning in computer vision. They have learned a rich hierarchy of features—from simple edges and textures in the early layers to complex object parts and patterns in the later layers—that are broadly applicable to many different visual recognition tasks.

<br>

#### **3.2. Application Strategies**

There are two primary strategies for applying a pre-trained ResNet to a new dataset:

1.  **ResNet as a Feature Extractor:**
    * **Process:** The entire pre-trained model is loaded, except for its final fully connected layer (the classifier). The weights of all the convolutional layers are **frozen**, meaning they are not updated during training.
    * The new dataset's images are passed through this frozen convolutional base to obtain feature vectors (activations from the final pooling layer).
    * A new, small classifier (e.g., a single fully connected layer or a small multi-layer perceptron) is trained from scratch using these extracted features as input.
    * **When to use:** This approach is best when the new dataset is small and very similar to the original dataset (e.g., ImageNet). Freezing the weights prevents overfitting on the small dataset.

2.  **Fine-Tuning the ResNet Model:**
    * **Process:** The pre-trained model is loaded, and its classifier head is replaced with a new one suited for the new task.
    * Instead of freezing the entire convolutional base, we unfreeze some of the later layers and train them along with the new classifier. The earlier layers, which learn more generic features, are often kept frozen.
    * The unfrozen layers and the new classifier are trained on the new dataset using a very **low learning rate**. This is crucial to ensure that the pre-trained weights are only slightly adjusted ("fine-tuned") to the new data, rather than being completely destroyed.
    * **When to use:** This is the most common and effective approach. It is suitable when the new dataset is of a reasonable size but may differ from the original dataset. Fine-tuning allows the model to adapt its more task-specific features (in the later layers) to the nuances of the new problem.