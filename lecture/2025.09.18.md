# Lecture 06 Notes: A Comparative Study of Optimization Algorithms

## 1. Beyond the Fixed Learning Rate

Until now, we have used a single, fixed learning rate ($\eta$) for the entire training process. However, this is suboptimal.
- A **large LR** is beneficial at the start of training, allowing the model to take large steps and quickly approach a good area of the loss landscape. But later in training, it can cause the loss to oscillate around the minimum without ever settling.
- A **small LR** allows the model to carefully explore a region and find the precise bottom of a loss "valley". But if used from the beginning, training would be incredibly slow.

The ideal strategy is to start with a larger learning rate and gradually decrease it as training progresses. This is the core idea behind **Learning Rate Scheduling**.

<br>

### Learning Rate Scheduling
A Learning Rate Scheduler modifies the learning rate during training according to a pre-defined rule. This can lead to faster convergence and better final model performance.

**Common Scheduler Types:**
- **Step Decay**: The LR is reduced by a factor (e.g., multiplied by 0.1) at specific, pre-defined epochs. It's simple and very effective.
- **Exponential Decay**: The LR is multiplied by a small decay factor after every epoch, leading to a smooth, exponential decrease.
- **Cosine Annealing**: The LR follows the shape of a cosine curve, starting high and smoothly decreasing to a minimum value. It has become very popular as it allows for robust exploration in the later stages of training.

Schedulers are an additional tool that can be used on top of any of the optimizers discussed below.

<br>
<br>

## 2. A Comparative Look at Optimizers

While the learning rate is crucial, the algorithm that *uses* the learning rate to update weights—the optimizer—is just as important. Different optimizers can be visualized as different strategies for descending a complex, hilly terrain (the loss landscape).

### A. Stochastic Gradient Descent (SGD)
This is the most fundamental optimization algorithm.
- **Intuition**: It's like trying to walk down a mountain in a thick fog. You can only see the slope of the ground right under your feet, so you take a small step in the steepest downward direction. It's a simple, but not always efficient, strategy.
- **Update Rule**: For a parameter $\theta$, the update is simply:

$$
\theta = \theta - \eta \cdot \nabla_{\theta}L(\theta)
$$

- **Pros**: Computationally light and easy to understand.
- **Cons**:
    - **Slow Convergence**: Progress can be very slow in "ravines"—long, narrow valleys in the loss landscape where the gradient in one direction is much steeper than in the other.
    - **High Oscillation**: The noisy gradient estimates from small batches can cause the path to the minimum to be very erratic. 
    - Prone to getting stuck in poor local minima.

<br>

### B. SGD with Momentum
Momentum was introduced to address the shortcomings of SGD.
- **Intuition**: It's like giving the ball rolling down the hill some mass. Instead of just following the local gradient, it builds up "velocity" in directions where the gradient is persistent. This helps it to blast through small local minima and dampens the oscillations in ravines, accelerating convergence.
- **Update Rule**: It introduces a velocity term $v_t$ which is an exponentially decaying moving average of past gradients.

$$
v_t = \beta v_{t-1} + \eta \cdot \nabla_{\theta}L(\theta)
$$

$$
\theta = \theta - v_t
$$

  Here, $\beta$ is the momentum coefficient, typically set to a value like 0.9.
- **Pros**: Significantly faster convergence than standard SGD. Smoother descent path.
- **Cons**: Introduces one more hyperparameter ($\beta$) to tune.

<br>

### C. Adam (Adaptive Moment Estimation)
Adam is currently the most widely used optimizer in deep learning, often the default choice.
- **Intuition**: Adam is a sophisticated algorithm that combines the idea of momentum with adaptive learning rates. It computes individual, adaptive learning rates for each parameter. It's like an expert hiker who not only uses momentum to keep a steady pace but also adjusts their step size for each foot based on the slipperiness and steepness of the terrain under it.
- **Update Rule (Conceptual)**:
  1. It keeps an exponentially decaying average of past gradients (like momentum, the **1st moment**).
  2. It keeps an exponentially decaying average of past *squared* gradients (the **2nd moment**).
  3. These averages are used to update the parameters, with the learning rate being scaled on a per-parameter basis. Parameters with larger past gradients get smaller updates.
- **Pros**:
    - Combines the benefits of Momentum and other adaptive techniques (like RMSprop).
    - Converges very quickly in practice.
    - Generally robust and works well with default hyperparameter values ($\beta_1=0.9, \beta_2=0.999$).
- **Cons**: Can be computationally more intensive than SGD. Some research suggests that in certain cases, a well-tuned SGD+Momentum can find a better final minimum, though Adam will usually get to a "good" solution faster.

<br>
<br>

## 3. The Experimental Comparison: Visualizing Performance

To truly understand the differences, we can run an experiment.
- **Setup**: Take one model architecture and one dataset. Train the model multiple times from the same starting point, changing only the optimizer for each run (e.g., SGD, Momentum, Adam).
- **Analysis via Visualization**: Plot the training loss and validation accuracy curves for all optimizers on the same graphs. 

- **Interpreting the Loss Curve**:
  - **SGD**: You would expect to see a noisy curve that decreases slowly.
  - **Momentum**: The curve will be smoother than SGD's and will likely descend faster.
  - **Adam**: This curve will typically show the most rapid initial decrease in loss.

- **Interpreting the Accuracy Curve**:
  - The accuracy curves will generally be the inverse of the loss curves.
  - Adam will likely achieve a high accuracy much faster than the others.
  - It's possible for a well-tuned SGD+Momentum to eventually overtake Adam and achieve a slightly higher peak accuracy, but this often requires more epochs and careful LR tuning.

This experiment provides a clear visual intuition for the strengths and weaknesses of each algorithm, confirming why Adam is often the preferred starting point for many deep learning tasks.