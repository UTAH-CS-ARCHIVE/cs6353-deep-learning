## Week 7: Convolutional Neural Networks (CNN) I - Core Architecture

### **Part 1: The Principles of CNNs for Image Processing**

#### **Why CNNs for Images?**

In previous weeks, we discussed Multi-Layer Perceptrons (MLPs). While an MLP can technically process an image (by flattening it into a single long vector), this approach is highly inefficient and flawed.

1.  **Loss of Spatial Structure:** Flattening an image completely destroys the spatial hierarchy. Pixels that are close to each other in a 2D grid are highly related, but this information is lost when we unroll the image into a 1D vector. An MLP treats a pixel in the top-left corner and a pixel in the bottom-right corner as being equally related.
2.  **Parameter Explosion:** Consider a small 256x256 pixel color image. The input layer would have $256 \times 256 \times 3 = 196,608$ neurons. If the first hidden layer has, say, 1000 neurons, the number of weights required for this single layer would be over 196 million. This makes the model computationally expensive, slow to train, and extremely prone to overfitting.

**Convolutional Neural Networks (CNNs)** are a specialized type of neural network designed to process grid-like data, such as images. They use two clever ideas—convolution and pooling—to preserve spatial relationships and drastically reduce the number of parameters.

<br>

#### **Core Operations in a CNN**

1.  **Convolution:**
    This is the main building block of a CNN. Instead of connecting every input neuron to every output neuron, a CNN uses a small filter (or **kernel**) that slides over the input image. This filter is essentially a small matrix of weights. At each position, the filter performs an element-wise multiplication with the patch of the image it's currently on and sums the results to produce a single output value. This process is repeated across the entire image, generating a 2D output called a **feature map**.
    
    The key idea is that the filter learns to detect a specific local feature, like a vertical edge, a corner, or a patch of a certain color. Because the same filter is applied everywhere, it can detect this feature regardless of where it appears in the image. A single convolutional layer will typically learn many such filters in parallel, each one specializing in detecting a different feature.

    

2.  **Pooling:**
    After a convolution operation, it's common to apply a **pooling** (or subsampling) layer. The goal of pooling is to progressively reduce the spatial size (width and height) of the feature maps, which in turn reduces the number of parameters and computation in the network. It also helps to make the learned features more robust to small translations and distortions in the input image.
    
    The most common type is **Max Pooling**. A small window (e.g., 2x2) slides over the feature map, and at each step, it takes the *maximum* value from that window. This effectively summarizes the features in a neighborhood, retaining the most prominent ones.

    

<br>

---

<br>

### **Part 2: The Mathematical Foundation of Convolution**

#### **The Convolution Operation**

Mathematically, the operation used in CNNs is a **discrete convolution**. For a 2D image $I$ and a 2D kernel $K$, the convolution is defined as:
$$
(I * K)(i, j) = \sum_m \sum_n I(m, n) K(i-m, j-n)
$$
This formula describes the value of the output feature map at position $(i, j)$. It's a sum of products between the input pixels and the kernel's values.

A key detail in this formal definition is that the kernel is **flipped** (rotated by 180 degrees) before being applied to the input. This property is mathematically useful in signal processing. However, in deep learning, this flipping operation is unnecessary for learning features and is computationally inconvenient.

Therefore, deep learning libraries actually implement a slightly different operation called **cross-correlation**, where the kernel is *not* flipped:
$$
(I \star K)(i, j) = \sum_m \sum_n I(i+m, j+n) K(m, n)
$$
For the purposes of training a neural network, the distinction is irrelevant. Since the kernel's weights are learned during training, the network can simply learn a flipped version of the filter if that's what's optimal. For this reason, the term **"convolution" is used universally in the deep learning community to refer to what is technically cross-correlation**.

<br>

#### **Parameter Sharing and its Consequences**

The true genius of the convolutional layer lies in **parameter sharing**. In an MLP, every connection has its own unique weight. In a CNN, the weights in a single filter (e.g., a 3x3 filter has 9 weights + 1 bias) are the *only* parameters that need to be learned for that entire feature map.

This has two profound consequences:

1.  **Drastic Reduction in Parameters:** Instead of millions of weights, we might only need a few hundred or thousand for a convolutional layer, depending on the number of filters we choose. This makes CNNs far more computationally efficient and less prone to overfitting than MLPs for image tasks.

2.  **Translation Invariance (or Equivariance):** Because the same filter is used to scan the entire image, the network's ability to detect a feature is independent of its location. If a filter learns to detect a cat's ear, it will be able to detect that same ear whether it's in the top-left corner or the bottom-right corner of the image. This property is more accurately termed **translation equivariance**—if the input shifts, the feature map also shifts by the same amount. The pooling layers then help to achieve true **invariance**, where the final output is robust to small shifts in the input. This is an incredibly powerful and desirable property for object recognition tasks.