### **Course Notes: Week 2 (Advanced/Lab Session)**
**Topic: Implementing RNN-Based Text Generation Models**

***

### **Part 1: The Exploding Gradient Problem in RNNs**

#### **1.1. Context: Backpropagation Through Time (BPTT)**

Recurrent Neural Networks (RNNs) are trained using a variation of backpropagation called Backpropagation Through Time (BPTT). Because an RNN processes a sequence, the network is conceptually "unrolled" in time for the length of the sequence. The loss at the final time step depends on the computations at all previous time steps.

During BPTT, the gradient of the loss function $L$ with respect to the weights is calculated by applying the chain rule backward from the last time step to the first. A critical part of this calculation involves computing the gradient of the loss with respect to the hidden state at each time step, $\frac{\partial L}{\partial h_t}$. This gradient depends on the gradient from the next time step, $\frac{\partial L}{\partial h_{t+1}}$.

The relationship can be expressed as:
$$
\frac{\partial L_t}{\partial h_{t-1}} = \frac{\partial L_t}{\partial h_t} \frac{\partial h_t}{\partial h_{t-1}}
$$
When propagated across many time steps, this involves the repeated multiplication of the Jacobian matrix $\frac{\partial h_t}{\partial h_{t-1}}$.

<br>

#### **1.2. The Cause of Exploding Gradients**

The hidden state in a simple RNN is updated as $h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$. The Jacobian matrix $\frac{\partial h_t}{\partial h_{t-1}}$ is therefore a function of the recurrent weight matrix $W_{hh}$ and the derivative of the activation function.

When the gradient is backpropagated through $k$ time steps, the calculation will involve a term proportional to $(W_{hh})^k$. If the spectral radius (the largest absolute eigenvalue) of the weight matrix $W_{hh}$ is greater than 1, the repeated multiplication will cause the norm of the gradient to grow exponentially.

This exponential growth is known as the **exploding gradient problem**. It results in extremely large gradients, which in turn cause massive updates to the model's weights during training.

<br>

#### **1.3. Consequences**

* **Training Instability:** The large weight updates can cause the optimizer to overshoot any minima, making the model parameters oscillate wildly and preventing convergence.
* **Numerical Overflow:** The gradient values can become so large that they exceed the floating-point precision of the hardware, resulting in `NaN` (Not a Number) values in the loss and weights, effectively halting the training process.

***

### **Part 2: Gradient Clipping - A Solution to Exploding Gradients**

Gradient clipping is a simple but highly effective technique used to mitigate the exploding gradient problem. It doesn't solve the theoretical issue but provides a practical way to keep training stable.

#### **2.1. The Core Idea**

The technique involves setting a predefined threshold for the magnitude (norm) of the gradients. If the norm of the gradients exceeds this threshold after a backward pass, the gradients are rescaled to match the threshold. This ensures that the size of the weight update step is always bounded.

Importantly, gradient clipping **only changes the magnitude** of the gradient vector, not its direction. The optimization step is still taken in the correct direction of the steepest descent, just with a smaller step size.

<br>

#### **2.2. The Algorithm**

The most common form is L2 norm clipping. The process is as follows:
1.  After the gradients $\nabla_{\theta} L$ are computed for all model parameters $\theta$ during the backward pass, calculate the global L2 norm of the entire gradient vector:
    $$
    g = ||\nabla_{\theta} L||_2 = \sqrt{\sum_{i} (\nabla_{\theta_i} L)^2}
    $$
2.  Define a hyperparameter, the clipping `threshold`, which we'll call $\tau$. This is a value you set before training begins (e.g., 1.0, 5.0).
3.  Compare the computed norm $g$ with the threshold $\tau$.
4.  If $g > \tau$, rescale the gradients:
    $$
    \text{Clipped Gradient} = \nabla_{\theta} L \times \frac{\tau}{g}
    $$
    This operation shrinks the gradient vector so that its L2 norm is exactly equal to $\tau$.
5.  If $g \le \tau$, the gradients are left unchanged.
6.  Use the (potentially clipped) gradients to update the model weights.

This procedure is applied before each weight update step of the optimizer.

***

### **Part 3: Practical Focus - Character-Level RNN for Text Generation**

This practical exercise involves building an RNN that learns to generate text one character at a time.

#### **3.1. Model Overview**

The model's task is to predict the most likely next character in a sequence, given all the characters that came before it.
* **Input:** A sequence of characters.
* **Output:** A probability distribution over the entire vocabulary for the next character in the sequence.
* **Vocabulary:** The set of all unique characters present in the training text data.

<br>

#### **3.2. Step-by-Step Process**

1.  **Data Preparation:**
    * First, we read our training corpus (a plain text file).
    * We create the vocabulary by finding all unique characters in the corpus.
    * We then build two dictionaries for mapping: `char_to_int` for converting characters to unique integer indices, and `int_to_char` for the reverse mapping.

2.  **Preparing Training Data:**
    * The model is trained on input-target pairs. We slide a window over the entire text corpus to create sequences. For an input sequence of characters, the target sequence is the same sequence, but shifted one character to the right.
    * *Example*: If the text is "hello" and sequence length is 4, one input-target pair would be:
        * Input (X): "hell"
        * Target (y): "ello"
    * Each character (represented by its integer index) is then converted into a one-hot vector. The size of this vector is equal to the vocabulary size.

3.  **Model Architecture & Forward Pass:**
    * The model consists of an RNN layer (e.g., a basic RNN, LSTM, or GRU cell) followed by a fully connected (Dense) output layer.
    * At each time step $t$:
        * The one-hot vector for the input character $x_t$ is fed into the RNN cell.
        * The RNN cell computes the current hidden state $h_t$ using the input $x_t$ and the previous hidden state $h_{t-1}$.
        * The hidden state $h_t$ is passed to the Dense layer, which outputs a vector of logits (raw scores) with a size equal to the vocabulary size.
        * A Softmax function is applied to these logits to produce a probability distribution $p_t$ over all possible next characters.

4.  **Training and Prediction Tracking:**
    * During training, we use the cross-entropy loss to compare the model's predicted probability distribution $p_t$ with the actual next character (represented as a one-hot vector).
    * The loss is calculated at each time step and then averaged or summed over the sequence.
    * BPTT is performed to get the gradients, **gradient clipping** is applied, and an optimizer updates the weights.
    * By tracking the model's output at each step, we can observe how its predictions evolve. For the input "hell", we can see the probability distribution it generates after seeing 'h', then after seeing 'he', then 'hel', and so on. We expect the probability for 'e' to be high after 'h', 'l' to be high after 'he', etc., as the model learns the patterns in the language.

5.  **Text Generation (Inference):**
    * To generate new text, we start with a "seed" string (e.g., "The ").
    * This seed is processed by the model to produce an initial hidden state.
    * We then take the very last character of the seed and feed it into the model to get a probability distribution for the next character.
    * We sample a character from this distribution (either by taking the most likely one or by sampling probabilistically to introduce randomness).
    * The sampled character is appended to our generated text and then used as the input for the next time step. This process is repeated to generate text of the desired length.