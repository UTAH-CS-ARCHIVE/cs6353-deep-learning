# [Lecture Note] Implementing Single-Layer Neural Networks

> Written by Hongseo Jang

## 1. Core Components of a Model in PyTorch/TensorFlow

Training a neural network involves defining three essential components: the **Model**, the **Loss Function**, and the **Optimizer**. This structure is fundamental to nearly all deep learning tasks and is conceptually identical in both PyTorch and TensorFlow.

<br>

### 1.1. The Model
The model defines the architecture of our neural network—how the layers are structured and how data flows from input to output (the forward pass). For a single-layer network, this is typically a single linear transformation.

- **Model Definition**: A model is essentially a function that takes an input `x` and transforms it using its parameters (weights `W` and bias `b`) to produce an output `ŷ`.

- **Linear Regression Model**: A simple linear layer. The model equation is:

  $$
  \hat{y} = Wx + b
  $$

  In PyTorch, we define this using `torch.nn.Linear`. In TensorFlow (with Keras), this is `tf.keras.layers.Dense`.

- **Logistic Regression Model**: This consists of a linear layer followed by a sigmoid activation function. The sigmoid squashes the output to a range between 0 and 1, which can be interpreted as a probability.

  $$
  \hat{y} = \sigma(Wx + b) = \frac{1}{1 + e^{-(Wx+b)}}
  $$

  This is implemented by adding a `torch.nn.Sigmoid` layer (PyTorch) or using `activation='sigmoid'` in the `Dense` layer (TensorFlow).

<br>

### 1.2. The Loss Function (Criterion)
The loss function quantifies how far the model's prediction (`ŷ`) is from the true label (`y`). The goal of training is to minimize this value.

- **For Linear Regression**: **Mean Squared Error (MSE)** is the standard choice. It computes the average of the squared differences between predictions and actual values.

  $$
  L_{MSE} = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2
  $$

  This is available as `nn.MSELoss()` in PyTorch and `tf.keras.losses.MeanSquaredError()` in TensorFlow.

- **For Logistic Regression**: **Binary Cross-Entropy (BCE)** is used. It is specifically designed for binary (0/1) classification problems.
  
  $$
  L_{BCE} = -\frac{1}{N}\sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]
  $$

  This is `nn.BCELoss()` in PyTorch and `tf.keras.losses.BinaryCrossentropy()` in TensorFlow.

<br>

### 1.3. The Optimizer
The optimizer is the algorithm used to update the model's parameters (`W` and `b`) to minimize the loss. It uses the gradients computed during backpropagation to take "steps" in the right direction on the loss surface.

- **Gradient Descent**: This is the core idea. The update rule for any parameter $\theta$ is:

  $$
  \theta_{new} = \theta_{old} - \eta \cdot \nabla_{\theta}L
  $$
  
  Here, $\eta$ is the **learning rate**, and $\nabla_{\theta}L$ is the gradient of the loss with respect to the parameter $\theta$.

- **Common Optimizers**: **Stochastic Gradient Descent (SGD)** is a fundamental optimizer. **Adam** is another very popular and often more effective choice.
  - In PyTorch: `torch.optim.SGD(model.parameters(), lr=0.01)`
  - In TensorFlow: `tf.keras.optimizers.SGD(learning_rate=0.01)`

<br>
<br>

## 2. The Training Process in Code

The training process is an iterative loop. In each step (or epoch), the model processes data and updates its weights.

### 2.1. The Training Loop Steps
The steps inside the loop are standard for supervised learning:

1.  **Forward Pass**: Compute the predicted output `ŷ` by passing the input data `X` through the model.
2.  **Compute Loss**: Use the loss function to measure the error between the predictions `ŷ` and the true labels `y`.
3.  **Compute Gradients (Backpropagation)**: Calculate the gradient of the loss with respect to each model parameter (e.g., $\frac{\partial L}{\partial W}$). In PyTorch, this is done via `loss.backward()`. In TensorFlow, this is often handled automatically within a `GradientTape` context.
4.  **Update Weights**: Apply the optimizer to update the model's parameters using the computed gradients. In PyTorch, this is `optimizer.step()`. In TensorFlow, this is `optimizer.apply_gradients()`.
5.  **(Important for PyTorch)** **Zero Gradients**: Before the next iteration, clear the old gradients so they don't accumulate. This is done with `optimizer.zero_grad()`.

By repeating these steps, the model's parameters are gradually adjusted, and we expect the loss value to decrease over time. Plotting the loss at each epoch is a crucial step to verify that the model is learning.



<br>
<br>

## 3. Observing the Effect of the Learning Rate (LR)

The learning rate ($\eta$) is a hyperparameter that controls the step size during optimization. Its value critically impacts training performance.

-   **Small Learning Rate** (e.g., `1e-5`): The model learns very slowly as the weight updates are tiny. Convergence takes a long time, and the model may get stuck in a suboptimal local minimum. The loss curve will show a very slow, gradual decrease.

-   **Good Learning Rate** (e.g., `1e-2`): The model learns efficiently. The loss decreases steadily and converges to a good minimum. This is the desired behavior.

-   **Large Learning Rate** (e.g., `1.0`): The weight updates are too large, causing the optimizer to "overshoot" the minimum of the loss function. This leads to the loss fluctuating erratically or even increasing. This phenomenon is called **divergence**. The loss curve will look unstable and may shoot upwards.

Experimenting with the learning rate is a fundamental part of training neural networks. Watching the loss curve is the primary method for diagnosing if your chosen learning rate is effective.