## Week 9: Recurrent Neural Networks (RNN)

### **Part 1: Handling Sequential Data**

#### **Why RNNs for Sequences?**

So far, we've looked at MLPs for general vector data and CNNs for grid-like data (images). However, many real-world datasets are **sequential** in nature:
-   **Natural Language:** A sentence is a sequence of words. The order of words is critical to the meaning.
-   **Time Series Data:** Stock prices, weather measurements, or sensor readings are sequences of values over time.
-   **Speech and Music:** These are sequences of audio signals.

Feedforward networks like CNNs and MLPs are not designed for this. They process a fixed-size input and produce a fixed-size output, assuming that all inputs are independent of each other. They have no inherent concept of "time" or "order."

**Recurrent Neural Networks (RNNs)** are a class of neural networks specifically designed to handle sequential data. The core idea is that they have a "memory" that captures information about what has been processed so far. They achieve this by having connections that form a directed cycleâ€”the output of a layer is fed back into itself as an input for the next step. This "loop" allows information to persist.

An RNN processes a sequence one element at a time. At each step, it takes the current input and its own output from the previous step (its "memory") to produce the new output.



<br>

---

<br>

### **Part 2: The Mathematics of Recurrence and Training**

#### **The Recurrence Relation**

The "loop" in an RNN is defined by a simple but powerful recurrence relation. At each time step $t$, the network's **hidden state** $h_t$ is updated based on two things: the previous hidden state $h_{t-1}$ and the current input $x_t$.

The equations that govern a simple RNN cell are:
1.  **Hidden State Update:**
    $$
    h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
    $$
    Let's break this down:
    -   $h_t$: The new hidden state at the current time step $t$. This vector acts as the network's memory.
    -   $h_{t-1}$: The hidden state from the previous time step $t-1$.
    -   $x_t$: The input vector at the current time step $t$.
    -   $W_{hh}$: The weight matrix applied to the previous hidden state.
    -   $W_{xh}$: The weight matrix applied to the current input.
    -   $b_h$: The bias for the hidden state.
    -   $\tanh$: The activation function, typically the hyperbolic tangent, is used to introduce non-linearity.

2.  **Output Calculation:**
    $$
    y_t = W_{hy} h_t + b_y
    $$
    -   $y_t$: The output of the network at time step $t$.
    -   $W_{hy}$: The weight matrix that maps the hidden state to the output.
    -   $b_y$: The bias for the output.

Crucially, the weight matrices ($W_{hh}, W_{xh}, W_{hy}$) and biases ($b_h, b_y$) are the **same for all time steps**. This is the equivalent of "parameter sharing" for CNNs, but across the time dimension. The network learns a single set of rules that it applies at every step of the sequence.

<br>

#### **BPTT (Backpropagation Through Time)**

How do we train an RNN? We can't use standard backpropagation directly on the looped graph. The solution is to **unroll** the network through time. We create a full, deep feedforward network where each time step is a new layer. For a sequence of length T, we get a T-layer deep network.

[Image comparing a looped RNN and its unrolled version]

Once unrolled, we can use a slightly modified version of backpropagation called **Backpropagation Through Time (BPTT)**. The process is:
1.  Feed the entire input sequence into the unrolled network and compute the outputs ($y_1, y_2, ..., y_T$).
2.  Calculate the loss at each time step and sum them up to get the total loss for the sequence.
3.  Calculate the gradients for the shared weight matrices by backpropagating the error from the end of the sequence ($y_T$) all the way back to the beginning ($y_1$). Since the weights are shared across time steps, the final gradient for a weight matrix (e.g., $W_{hh}$) is the sum of its gradients calculated at each time step.
4.  Update the weights using the total gradients.

<br>

#### **The Long-Term Dependency Problem**

BPTT has a major weakness. Because the unrolled network is essentially a very deep network, it suffers severely from the **vanishing and exploding gradient problems** that we discussed in Week 4.

When a sequence is very long, the gradients have to be propagated back through many time steps. During this process, they are repeatedly multiplied by the hidden-to-hidden weight matrix, $W_{hh}$.
-   If the values in $W_{hh}$ are small, the gradients will shrink exponentially and vanish, making it impossible for the network to learn connections between distant events. This is the **Long-Term Dependency Problem**: the model struggles to connect information from the beginning of a long sentence to its end.
-   If the values are large, the gradients will grow exponentially and explode, leading to unstable training.

This fundamental limitation of simple RNNs makes it very difficult for them to learn from long sequences, which led to the development of more advanced architectures like LSTMs and GRUs.