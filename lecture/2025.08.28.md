## [Lecture Note] Neural Network Basics: Perceptron & Loss Functions

> Written by Hongseo Jang

### **Part 1: From Traditional Models to Neural Networks**

#### **Relationship between Machine Learning and Neural Networks**

It's important to clarify the relationship between Machine Learning (ML) and Neural Networks (NNs). **Machine Learning** is a broad field of AI that gives computers the ability to learn from data without being explicitly programmed. It encompasses a wide variety of algorithms like Decision Trees, Support Vector Machines (SVMs), and k-Nearest Neighbors.

**Neural Networks** are a specific class of machine learning models, inspired by the structure of the human brain. Deep Learning, in turn, is a subfield of neural networks that focuses on networks with many layers (hence, "deep"). So, you can think of it as a set of nested dolls: AI is the largest, inside is Machine Learning, then Neural Networks, and finally Deep Learning. While NNs are just one tool in the ML toolkit, they have become incredibly dominant due to their remarkable performance on complex, unstructured data like images and text.

<br>

#### **The Evolution from Regression to the Perceptron**

The Perceptron, the simplest form of a neural network, wasn't created in a vacuum. It's a natural evolution from classical statistical models like Linear and Logistic Regression.

1.  **Linear Regression:**
    The goal of linear regression is to find the best-fitting straight line (or hyperplane in higher dimensions) to a set of data points. The hypothesis function is a simple linear combination of the inputs:
    
    $$
    H(x) = W^T x + b
    $$
    
    Here, $H(x)$ predicts a continuous numerical value (e.g., the price of a house). The model learns the weights $W$ and bias $b$ that minimize the difference between the predicted values and the actual values. However, its output is unbounded, which makes it unsuitable for classification tasks where we need a probability or a clear-cut decision.

2.  **Logistic Regression:**
    Logistic regression adapts the linear model for binary classification (e.g., spam or not spam). It addresses the unbounded output issue of linear regression by passing the linear part through a "squashing" function, the **sigmoid** (or logistic) function, denoted by $\sigma(z)$.
    
    $$
    \sigma(z) = \frac{1}{1 + e^{-z}}
    $$
    
    The sigmoid function maps any real-valued number into the range (0, 1), which can be interpreted as a probability. So, the hypothesis for logistic regression becomes:
    
    $$
    H(x) = \sigma(W^T x + b)
    $$
    
    This gives us the probability that the given input $x$ belongs to the positive class.

3.  **The Perceptron:**
    The Perceptron is conceptually very similar to logistic regression. It takes inputs, computes a weighted sum, adds a bias, and then applies an **activation function**. The key difference lies in the activation function. The original Perceptron used a simple **step function**, which outputs 1 if the input is above a certain threshold and 0 otherwise.
    
    $$
    \text{step}(z) = \begin{cases} 1 & \text{if } z > \theta \\ 0 & \text{otherwise} \end{cases}
    $$
    
    If we use the sigmoid function instead of the step function, the model becomes identical to logistic regression. Therefore, a **single neuron** (or a Perceptron) in a modern neural network can be seen as a logistic regression unit. A full neural network is then essentially a collection of these units, organized in layers, where the outputs of one layer become the inputs for the next. This layered structure allows the network to learn much more complex, non-linear patterns than a single logistic regression model ever could.

<br>

---

<br>

### **Part 2: The Core Components of Learning**

#### **1. The Hypothesis Function**

As we saw, the hypothesis function is the mathematical representation of our model. It defines the forward passâ€”how an input $x$ is mapped to an output prediction $\hat{y}$. For a single neuron, the general form is:

$$
H(x) = \sigma(W^T x + b)
$$

Let's break this down:
-   $x$: The input vector.
-   $W$: The vector of weights. Each weight $w_i$ represents the importance of the corresponding input feature $x_i$.
-   $b$: The bias term. It's an intercept that allows the decision boundary to be shifted, making the model more flexible. Without it, the decision boundary would have to pass through the origin.
-   $W^T x + b$: This is the linear combination of inputs, often called the "logit" or $z$.
-   $\sigma(\cdot)$: The **activation function**. It introduces non-linearity into the model. While the original Perceptron used a step function, modern networks use smooth, differentiable functions like Sigmoid, Tanh, or ReLU, which is crucial for gradient-based learning.

<br>

#### **2. The Loss Function (or Cost Function)**

The hypothesis function gives us a prediction, $\hat{y}$. Now we need a way to measure how wrong that prediction is compared to the true label, $y$. This is the job of the **loss function**, $L(y, \hat{y})$. It quantifies the model's error for a single training example. The **cost function** is the average of the loss over the entire training dataset.

-   **Mean Squared Error (MSE):**
    MSE is the classic loss function for **regression** problems, where the goal is to predict a continuous value. It measures the average of the squared differences between the predicted and actual values.
    
    $$
    L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
    $$
    
    We square the error so that positive and negative errors don't cancel each other out, and it also penalizes larger errors more heavily.

-   **Cross-Entropy (or Log Loss):**
    MSE is not ideal for **classification** problems. For classification, we use Cross-Entropy. It measures the dissimilarity between the true distribution (the one-hot encoded true labels) and the predicted probability distribution. For binary classification (where labels are 0 or 1), the formula is:
    
    $$
    L = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]
    $$
    
    Let's analyze this:
    * If the true label $y_i=1$, the second term becomes zero, and the loss is $-\log(\hat{y}_i)$. We want $\hat{y}_i$ to be close to 1, and as $\hat{y}_i \to 1$, $-\log(\hat{y}_i) \to 0$. The loss is minimized.
    * If the true label $y_i=0$, the first term becomes zero, and the loss is $-\log(1-\hat{y}_i)$. We want $\hat{y}_i$ to be close to 0, and as $\hat{y}_i \to 0$, $-\log(1-\hat{y}_i) \to 0$. Again, the loss is minimized.
    
    Cross-entropy provides a steep gradient for confident but incorrect predictions, which helps the model learn faster.

<br>

#### **3. Optimization: The Goal of Training**

We have a hypothesis that makes predictions and a loss function that measures the error. The process of "learning" or "training" is simply an **optimization problem**.

The goal is to find the set of weights $W$ and biases $b$ that **minimize the cost function** $L(W, b)$.

Imagine the loss function as a huge, hilly landscape in a high-dimensional space, where each point's coordinates are the values of the weights and biases, and the altitude at that point is the corresponding loss. Our goal is to find the lowest point in this landscape. We do this using optimization algorithms like **Gradient Descent**, which we will cover in the next lecture. This involves calculating the gradient of the loss function with respect to the parameters ($W, b$) and taking small steps in the opposite direction of the gradient to descend into a valley of lower loss.