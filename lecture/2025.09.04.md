# [Lecture Note] Backpropagation from Scratch

> Written by Hongseo Jang

## 1. Autograd vs. Manual Backpropagation

In modern deep learning frameworks like PyTorch or TensorFlow, computing the gradients required to update our model's weights is as simple as calling a single function, like `loss.backward()`. This feature is known as **Automatic Differentiation (Autograd)**.

- **Autograd's Convenience**: It abstracts away the complex calculus, allowing us to build intricate models without manually deriving the gradients. The framework builds a computational graph during the forward pass and uses the chain rule to automatically compute gradients for all parameters during the backward pass.

- **Why Implement it Manually?**: Understanding what happens inside this "black box" is crucial for a deep learning practitioner.
  - **Debugging**: When a model isn't training correctly, understanding the flow of gradients can help diagnose issues like "vanishing" or "exploding" gradients.
  - **Customization**: If you want to implement a novel layer type or a custom loss function, you may need to define its custom backward pass.
  - **Fundamental Knowledge**: It solidifies the core theory behind how neural networks actually learn.

<br>
<br>

## 2. Forward Propagation in a 2-Layer MLP

Before we can go backward, we must go forward. The forward pass is the process of making a prediction. For a simple 2-Layer MLP (one hidden layer, one output layer), the steps are:

1.  **Input to Hidden Layer**: The input data `X` is linearly transformed by the first layer's weights `W1` and bias `b1`.

    $$
    Z_1 = X \cdot W_1 + b_1
    $$

2.  **Hidden Layer Activation**: A non-linear activation function, like Sigmoid ($\sigma$) or ReLU, is applied element-wise to $Z_1$. This introduces non-linearity, which is essential for learning complex patterns.

    $$
    A_1 = \sigma(Z_1)
    $$

3.  **Hidden to Output Layer**: The activated output from the hidden layer, $A_1$, is linearly transformed by the second layer's weights `W2` and bias `b2`.

    $$
    Z_2 = A_1 \cdot W_2 + b_2
    $$

4.  **Final Prediction**: A final activation function (e.g., Softmax for multi-class classification, or none for regression) is applied to get the final prediction, $\hat{Y}$.

    $$
    \hat{Y} = \text{softmax}(Z_2)
    $$

5.  **Calculate Loss**: The prediction $\hat{Y}$ is compared to the true labels `Y` using a loss function `L` (e.g., Cross-Entropy Loss). Our goal in backpropagation is to find out how to change `W1`, `b1`, `W2`, and `b2` to minimize this loss.

<br>
<br>

## 3. Backpropagation: The Chain Rule in Reverse

Backpropagation is simply a practical application of the **chain rule** from calculus. Its goal is to compute the gradient of the loss function with respect to each parameter in the network (e.g., $\frac{\partial L}{\partial W_1}$). It does this by starting from the end (the loss) and moving backward through the network, layer by layer.

The core idea is to calculate the "local" gradient of each operation and multiply it by the gradient of the loss flowing from the layer in front of it.

For example, to get the gradient for $W_1$, the chain rule looks like this:

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \hat{Y}} \cdot \frac{\partial \hat{Y}}{\partial Z_2} \cdot \frac{\partial Z_2}{\partial A_1} \cdot \frac{\partial A_1}{\partial Z_1} \cdot \frac{\partial Z_1}{\partial W_1}
$$

Backpropagation organizes this calculation efficiently.

<br>
<br>

## 4. The Backward Pass: Step-by-Step Gradient Calculation

Let's trace the gradients backward through our 2-Layer MLP.

1.  **Gradient of Loss w.r.t. Output ($Z_2$)**: The first step is to find how the loss changes with respect to the output of the final linear layer, $Z_2$. This gradient, $\delta_2 = \frac{\partial L}{\partial Z_2}$, is the starting point. For Cross-Entropy Loss with a Softmax activation, this derivative conveniently simplifies to $\hat{Y} - Y$.

2.  **Gradients for the Output Layer ($W_2, b_2$)**: Now, using the gradient $\delta_2$, we can find the gradients for the second layer's parameters.
    - **Weight Gradient**: $\frac{\partial L}{\partial W_2} = A_1^T \cdot \delta_2$
    - **Bias Gradient**: $\frac{\partial L}{\partial b_2} = \sum(\delta_2)$ (summed across the batch)

3.  **Propagate Gradient to Hidden Layer ($A_1$)**: Next, we need to find how the loss changes with respect to the output of the hidden layer activation, $A_1$. We propagate the gradient $\delta_2$ backward through $W_2$.
    - $\frac{\partial L}{\partial A_1} = \delta_2 \cdot W_2^T$

4.  **Gradient w.r.t. Hidden Layer Linear Output ($Z_1$)**: Now we account for the hidden layer's activation function. We multiply the incoming gradient by the derivative of the activation function ($\sigma'$).
    - $\delta_1 = \frac{\partial L}{\partial Z_1} = (\delta_2 \cdot W_2^T) * \sigma'(Z_1)$ (Note: $*$ is element-wise multiplication)

5.  **Gradients for the Hidden Layer ($W_1, b_1$)**: Finally, using the gradient $\delta_1$, we can find the gradients for the first layer's parameters.
    - **Weight Gradient**: $\frac{\partial L}{\partial W_1} = X^T \cdot \delta_1$
    - **Bias Gradient**: $\frac{\partial L}{\partial b_1} = \sum(\delta_1)$ (summed across the batch)

At this point, we have manually calculated the gradients for all parameters in the network.

<br>
<br>

## 5. Verification with Autograd

The final step in the practical exercise is to confirm our work.
1.  **Implement the above calculations** using only NumPy operations. This gives you a set of gradient matrices for $W_1, b_1, W_2, b_2$.
2.  **Build the identical 2-Layer MLP** in PyTorch or TensorFlow.
3.  **Perform the same forward pass** with the same initial weights and input data.
4.  **Call the autograd function** (`loss.backward()`).
5.  **Compare**: Print the gradient values stored by the framework (e.g., `W1.grad`) and compare them to your manually calculated NumPy gradient matrices. If they are numerically identical (within a small floating-point tolerance), your from-scratch implementation of backpropagation is correct. This process validates a true understanding of the algorithm.