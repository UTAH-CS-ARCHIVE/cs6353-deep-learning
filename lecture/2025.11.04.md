## Week 10: LSTM and GRU

### **Part 1: Solving the Long-Term Dependency Problem**

#### **The Challenge of Long-Term Memory**

Last week, we identified the critical weakness of simple RNNs: the **long-term dependency problem**. Due to the vanishing gradient issue in BPTT, RNNs struggle to capture relationships between events that are far apart in a sequence. For a task like sentence completion, the model might need to remember the subject from the beginning of a long paragraph to correctly predict a verb at the end. A simple RNN's "memory" (its hidden state) is too volatile for this; old information is quickly washed out by new inputs.

To solve this, more sophisticated recurrent architectures were developed. The two most successful and widely used are **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)**. Both are designed to selectively remember and forget information over long periods using a mechanism called "gating."

-   **LSTM (Long Short-Term Memory):** Proposed in 1997 by Hochreiter and Schmidhuber, LSTMs were a groundbreaking solution. They introduce a dedicated "memory line" called the **cell state**, which can carry information directly down the sequence with minimal distortion. The flow of information into and out of this cell state is carefully regulated by three "gates."

-   **GRU (Gated Recurrent Unit):** Introduced more recently by Cho et al. in 2014, the GRU is a simplified version of the LSTM. It combines the forget and input gates into a single "update gate" and merges the cell state and hidden state. This makes it computationally more efficient than an LSTM, with fewer parameters. In practice, its performance is often comparable to that of an LSTM, and the choice between them often depends on the specific dataset and task.

<br>

---

<br>

### **Part 2: The Mathematics of Gated Architectures**

#### **The Gate Mechanism**

The core innovation in both LSTM and GRU is the **gate**. A gate is a neural network layer (typically a sigmoid function) that controls how much information should be let through. It works like this:
1.  It takes some inputs (e.g., the previous hidden state and the current input).
2.  It passes them through a sigmoid activation function, which squashes the output to a value between 0 and 1.
3.  This output vector is then multiplied element-wise with another data vector.
    -   If a value in the gate's output is close to 0, it means "let nothing through" for the corresponding element in the data vector.
    -   If a value is close to 1, it means "let everything through."

This mechanism allows the network to *learn* what information to keep and what to discard.



<br>

#### **Inside the LSTM Cell**

An LSTM maintains two vectors that are passed from one time step to the next: the **hidden state** ($h_{t-1}$) and the **cell state** ($C_{t-1}$). The cell state is the long-term memory, and the hidden state is a filtered version of the cell state used to compute the output for the current time step. The magic lies in how three gates manipulate the cell state.

Let's analyze the equations for each gate at time step $t$. Note that $[h_{t-1}, x_t]$ denotes the concatenation of the previous hidden state and the current input.

1.  **Forget Gate ($f_t$):** This gate decides what information to throw away from the old cell state, $C_{t-1}$.
    $$
    f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
    $$
    The output $f_t$ is a vector of numbers between 0 and 1. It will be multiplied with $C_{t-1}$. A '1' means "keep this information," while a '0' means "completely forget this."

2.  **Input Gate ($i_t$ and $\tilde{C}_t$):** This gate decides what new information to store in the cell state. This is a two-step process:
    a. The sigmoid layer, called the "input gate layer," decides which values we'll update.
    $$
    i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
    $$
    b. A `tanh` layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state.
    $$
    \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
    $$

3.  **Cell State Update:** Now we update the old cell state $C_{t-1}$ into the new cell state $C_t$. We multiply the old state by the forget gate's output and add the new candidate values scaled by the input gate's output.
    $$
    C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
    $$
    This is the heart of the LSTM: we forget parts of the old state and add new, relevant information. Because this is an additive interaction (not multiplicative like in a simple RNN), it is much easier for gradients to flow through time without vanishing.

4.  **Output Gate ($o_t$):** Finally, this gate decides what to output as the new hidden state, $h_t$.
    a. The sigmoid layer decides which parts of the cell state weâ€™re going to output.
    $$
    o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
    $$
    b. We put the new cell state $C_t$ through a `tanh` function (to push the values between -1 and 1) and multiply it by the output of the sigmoid gate.
    $$
    h_t = o_t * \tanh(C_t)
    $$
    This $h_t$ is the final output for the current time step and is also passed on to the next time step.