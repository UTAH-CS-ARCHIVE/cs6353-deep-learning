## [Lecture Note] Multi-Layer Perceptron (MLP) & Backpropagation

> Written by Hongseo Jang

### **Part 1: Overcoming Limitations with Deeper Networks**

#### **The Limitation of the Single-Layer Perceptron: The XOR Problem**

Last week, we saw how the Perceptron is a powerful evolution from linear and logistic regression. However, a single-layer perceptron (which is essentially just a single neuron or a logistic regression model) has a critical limitation: it can only learn **linearly separable** patterns.

A classic example that illustrates this failure is the **XOR (exclusive OR)** problem. Let's consider the truth table for XOR:

| Input 1 | Input 2 | Output |
| :-----: | :-----: | :----: |
|    0    |    0    |   0    |
|    0    |    1    |   1    |
|    1    |    0    |   1    |
|    1    |    1    |   0    |

If we plot these points on a 2D graph, we can see the issue. There is no single straight line that can separate the points where the output is 1 (the 'true' class) from the points where the output is 0 (the 'false' class).



This is the core limitation. A single neuron works by defining a linear decision boundary (a line, a plane, or a hyperplane). Since XOR is not linearly separable, a single-layer perceptron is fundamentally incapable of solving it. This discovery, highlighted in the book "Perceptrons" by Minsky and Papert, was a major factor leading to the first "AI winter."

<br>

#### **The Solution: The Multi-Layer Perceptron (MLP)**

The solution is to add more layers of neurons. A **Multi-Layer Perceptron (MLP)** consists of at least three layers:
1.  An **Input Layer:** Receives the initial data.
2.  One or more **Hidden Layers:** These are the intermediate layers between the input and output. This is where the "magic" happens.
3.  An **Output Layer:** Produces the final prediction.

By adding a hidden layer, the network can learn non-linear combinations of the original features. The first layer might learn simple linear boundaries, but the hidden layer can then combine these lines to form more complex shapes, like a region, that can solve the XOR problem. For instance, a hidden layer can learn to represent "A OR B" and "NOT (A AND B)", and the output layer can then learn to AND these two intermediate representations together to get "XOR".

This ability to learn hierarchical features is what gives deep neural networks their power. Each layer learns to recognize increasingly complex patterns based on the outputs of the previous layer.

<br>

---

<br>

### **Part 2: The Engine of Learning - Backpropagation**

Okay, so we know we need multiple layers. But how do we train such a network? How do we figure out the correct weights and biases for all these interconnected neurons, especially those in the hidden layers? We can't directly measure their error.

This is where **Backpropagation** comes in. It is the core algorithm used to train MLPs and virtually all deep learning models.

#### **The Principle of Backpropagation**

Backpropagation, short for "backward propagation of errors," is a clever and efficient application of the **chain rule** from calculus.

Hereâ€™s the high-level idea:
1.  **Forward Pass:** We feed an input $x$ through the network, from the input layer to the output layer, to get a prediction $\hat{y}$.
2.  **Compute Loss:** We calculate the loss (e.g., using Cross-Entropy) by comparing the prediction $\hat{y}$ with the true label $y$.
3.  **Backward Pass:** This is the key step. We start at the output layer and calculate the gradient of the loss with respect to the weights of that layer. Then, we move backward to the previous layer. We use the chain rule to calculate how the loss changes with respect to that layer's weights, *reusing* the gradient information we just computed from the layer ahead.
4.  **Propagate the Error:** We continue this process, propagating the error signal backward, layer by layer, all the way to the input layer. At each layer, we compute the gradient of the loss function with respect to that layer's weights.
5.  **Update Weights:** Once we have the gradients for all weights in the network, we use an optimization algorithm (like Gradient Descent) to update the weights in the direction that minimizes the loss.

By systematically propagating the error backward, we can efficiently assign "blame" to each weight for its contribution to the total error and adjust it accordingly.

<br>

#### **The Mathematics of Backpropagation**

The goal is to compute the partial derivative of the loss function $L$ with respect to every weight $W$ in the network. Let's focus on the weights $W^{(l)}$ of a specific layer $l$.

The update rule for these weights is derived from the chain rule. The formula provided in the lecture slides is a compact representation of this process:

$$
\frac{\partial L}{\partial W^{(l)}} = \delta^{(l+1)} (a^{(l)})^T
$$

Let's dissect this equation:
-   $\frac{\partial L}{\partial W^{(l)}}$: This is the **gradient** we want to compute. It's a matrix that tells us how a small change in each weight in the matrix $W^{(l)}$ will affect the total loss $L$.

-   $a^{(l)}$: This is the **activation output** vector from the neurons in layer $l$. When we performed the forward pass, this is the value that layer $l$ computed and passed on to layer $l+1$. It's also the input to layer $l+1$.

-   $\delta^{(l+1)}$: This is the crucial **error term** (or delta term) for the *next* layer, $l+1$. This vector represents the error of the *pre-activation* outputs (the logits, $z^{(l+1)}$) of layer $l+1$. It's calculated by taking the gradient of the loss with respect to these logits, $\frac{\partial L}{\partial z^{(l+1)}}$. This term is what gets "propagated backward". The error at layer $l$ is computed using the error from layer $l+1$, which was computed using the error from layer $l+2$, and so on.

In essence, the formula tells us that the gradient for the weights of layer $l$ is the outer product of the error signal from the *next* layer ($\delta^{(l+1)}$) and the activation outputs from the *current* layer ($a^{(l)}$). This makes intuitive sense: the update for a weight connecting neuron $i$ in layer $l$ to neuron $j$ in layer $l+1$ should be proportional to both the activation of neuron $i$ (if the activation is zero, it didn't contribute, so the weight shouldn't change) and the error attributed to neuron $j$ (if that neuron's output wasn't causing any error, we don't need to change the weights leading into it).