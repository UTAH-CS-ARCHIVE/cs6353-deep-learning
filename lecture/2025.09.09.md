## [Lecture Note] Activation Functions & The Gradient Instability Problem

> Written by Hongseo Jang

### **Part 1: The Role of Activation Functions and Training Instability**

#### **Types and Characteristics of Activation Functions**

In previous weeks, we established that activation functions are a crucial component of a neuron, introducing the non-linearity that allows neural networks to learn complex patterns. Without them, a multi-layer network would just be a series of linear operations, which is equivalent to a single linear model.

Let's move beyond the simple step function and sigmoid and look at the functions commonly used today. The choice of activation function is a critical design decision that can significantly impact a network's performance and training dynamics.

-   **Sigmoid:** As we know, it squashes any real-valued input into the (0, 1) range.

    $$
    \sigma(x) = \frac{1}{1 + e^{-x}}
    $$

    While historically important and useful for binary classification output layers (to represent a probability), it has fallen out of favor for hidden layers.
    
-   **Tanh (Hyperbolic Tangent):** This function squashes inputs into the (-1, 1) range.

    $$
    \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    $$

    It's essentially a scaled and shifted version of the sigmoid function. Its output is zero-centered, which is a desirable property that tends to make learning a bit easier for the subsequent layers compared to the non-zero-centered sigmoid. However, like sigmoid, it suffers from a major drawback.

-   **ReLU (Rectified Linear Unit):** This is the most popular activation function for hidden layers in deep learning today. Its definition is elegantly simple:

    $$
    f(x) = \max(0, x)
    $$

    It returns the input directly if it's positive, and returns zero otherwise. Its simplicity and effectiveness were a major breakthrough in making deep networks practical. We'll explore why in the mathematical section.

There are many other variants like Leaky ReLU, Parametric ReLU (PReLU), and Exponential Linear Unit (ELU) that aim to improve upon ReLU, especially to solve some of its weaknesses.

<br>

#### **The Problem of Unstable Training**

Training a very deep neural network is a difficult task. The core challenge comes from the backpropagation process itself. As we propagate the error gradient from the output layer all the way back to the input layer, we are repeatedly multiplying small numbers by the chain rule. This repeated multiplication can lead to two infamous problems:

1.  **Vanishing Gradients:** The gradients become exponentially smaller as they are propagated backward. As a result, the weights of the initial layers learn very slowly, or stop learning altogether, because the updates they receive are close to zero. The network effectively becomes untrainable beyond a certain depth.

2.  **Exploding Gradients:** The opposite can also happen. The gradients become exponentially larger, leading to huge weight updates. This causes the training process to become unstable and diverge, often resulting in numerical overflows (represented as `NaN` - Not a Number - during training).

These issues made it incredibly difficult to train networks with more than a few layers, which was a major bottleneck in the history of deep learning.

<br>

---

<br>

### **Part 2: A Mathematical Look at Gradient Problems**

#### **The Issue with Sigmoid and Tanh**

The reason Sigmoid and Tanh cause the vanishing gradient problem lies in their derivatives.
Let's look at the derivative of the sigmoid function, $\sigma(x)$:

$$
\frac{d}{dx}\sigma(x) = \sigma(x)(1 - \sigma(x))
$$

This derivative has a maximum value of **0.25** (when $x=0$). The derivative of the Tanh function is slightly better, with a maximum value of **1.0** (when $x=0$).

Now, consider the chain rule during backpropagation. To get the gradient for a weight in an early layer, you multiply the gradients of all subsequent layers. This means you're multiplying many numbers that are at most 0.25 (for sigmoid) or 1.0 (for Tanh). If many of these values are less than 1, their product will shrink exponentially towards zero as the number of layers increases. For example, $(0.25)^{10} \approx 10^{-6}$. The signal of the error vanishes before it can reach the early layers.



This is the mathematical root of the **Vanishing Gradient** problem. The flat "saturated" regions of these functions (where the output is close to 0/1 for sigmoid or -1/1 for Tanh) have near-zero gradients, which exacerbates the issue.

<br>

#### **ReLU to the Rescue (and its own problem)**

ReLU's popularity stems directly from how it addresses the vanishing gradient problem.
The derivative of ReLU is:

$$
f'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
$$

For any positive input, the derivative is **1**. When we multiply gradients during backpropagation, multiplying by 1 doesn't diminish the signal. This allows the error signal to propagate much more effectively to earlier layers, enabling the training of much deeper networks.

However, ReLU is not a perfect solution. It has its own issue: the **Dying ReLU** problem. If a neuron's input is consistently negative, its output will be zero. Consequently, the gradient flowing through that neuron will also be zero. This means the weights of that neuron will never be updated again, because the gradient term will always be zero. The neuron is effectively "dead" and will not participate in the learning process anymore. This can happen if the learning rate is too high, causing a large weight update that pushes the neuron into a state where it only receives negative inputs.

<br>

#### **Vanishing/Exploding Gradients: A Deeper Look**

We can understand this problem more formally by considering the **Jacobian matrix**, which is the matrix of all first-order partial derivatives of a vector-valued function. In backpropagation, the gradient is repeatedly multiplied by the Jacobian of each layer's activation function.

Let's simplify and consider a deep linear network without activation functions. The gradient at an early layer is the product of all the weight matrices from the subsequent layers: $W_L, W_{L-1}, ..., W_{l+1}$. The repeated multiplication by these matrices determines the stability.

The behavior of this product is governed by the **eigenvalues** of these matrices.
-   If the eigenvalues of the Jacobian matrices are consistently less than 1, the gradient vector will shrink with each multiplication, leading to **Vanishing Gradients**.
-   If the eigenvalues are consistently greater than 1, the gradient vector will grow with each multiplication, leading to **Exploding Gradients**.

Maintaining eigenvalues close to 1 is key to stable training in deep networks. This mathematical insight has led to better initialization schemes (like Xavier/Glorot and He initialization) and network architectures (like Residual Networks and LSTMs with gating mechanisms) that are specifically designed to combat the vanishing and exploding gradient problems.