## Week 6: Regularization for Preventing Overfitting

### **Part 1: The Problem of Overfitting**

#### **What is Overfitting?**

One of the most common pitfalls in machine learning is **overfitting**. This occurs when a model learns the training data *too well*. Instead of capturing the general underlying patterns in the data, it starts to memorize the noise and random fluctuations specific to that training set.

The result is a model that performs exceptionally well on the data it was trained on but fails to **generalize** to new, unseen data. The model has high **variance**.



You can visualize this with a simple classification problem. An underfitting model might draw a simple line that fails to capture the data's structure. A good model will draw a smooth curve that separates the classes well. An overfitted model, however, will draw a highly complex, wiggly boundary that contorts itself to perfectly classify every single training point, including the outliers. This complex boundary is unlikely to be effective for new data points.

Overfitting is especially a problem in deep learning because neural networks have a very high capacity (millions of parameters). With so many parameters, they are flexible enough to memorize the training data easily. Regularization techniques are designed to constrain this flexibility, forcing the model to learn simpler, more generalizable patterns.

<br>

---

<br>

### **Part 2: Regularization Techniques**

Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.

<br>

#### **1. L1 & L2 Regularization**

This is one of the most common forms of regularization. The core idea is to add a **penalty term** to the loss function that discourages the model's weights from becoming too large. We modify the original loss function $L$ to become a regularized loss $L_{reg}$.

-   **L2 Regularization (Weight Decay):**
    L2 regularization adds a penalty proportional to the **squared magnitude** of the weights. The regularized loss function is:
    
    $$
    L_{reg} = L + \frac{\lambda}{2m} \sum ||W||^2_2
    $$
    
    Let's break it down:
    * $L$ is the original loss (e.g., cross-entropy).
    * $||W||^2_2$ is the squared L2 norm of the weight matrix, which is just the sum of the squares of all weight values.
    * $\lambda$ is the **regularization parameter**, a hyperparameter that controls the strength of the penalty. A higher $\lambda$ forces the weights to be smaller.
    * $2m$ is a scaling factor (where $m$ is the batch size).
    
    When we compute the gradient for the weight update, this penalty term adds a component that is proportional to $W$ itself. This leads to the update rule effectively shrinking the weights towards zero in each step, which is why it's also known as **Weight Decay**. By keeping the weights small, L2 regularization prevents the model from relying too heavily on any single feature and encourages a simpler, more distributed weight configuration, which tends to generalize better.

-   **L1 Regularization:**
    L1 regularization is similar, but it penalizes the **absolute value** of the weights (the L1 norm). This has the interesting effect of encouraging **sparsity**â€”it pushes many of the weights to be exactly zero, effectively performing feature selection within the model.

<br>

#### **2. Dropout**

**Dropout** is a brilliantly simple yet powerful regularization technique developed specifically for neural networks.

The idea is this: during training, for each forward pass, we **randomly "drop out"** (i.e., temporarily deactivate) a certain fraction of the neurons in a layer. This means their output is set to zero for that pass. The dropped-out neurons are different for each training batch.



How does this prevent overfitting?
1.  **Prevents Co-adaptation:** By randomly deactivating neurons, we prevent them from becoming overly reliant on each other. A neuron cannot depend on the presence of another specific neuron, because it might be dropped out at any moment. This forces each neuron to learn more robust features that are useful on their own.
2.  **Approximates Ensemble Learning:** Training a network with dropout is like training an exponential number of smaller, different neural networks simultaneously. At test time, we don't drop any neurons; instead, we scale down the activations by the dropout probability. This is an approximation of taking the average prediction from this huge ensemble of smaller networks. Ensemble methods are a very powerful way to reduce overfitting, and dropout provides an efficient way to approximate this effect.

<br>

#### **3. Batch Normalization (Batch Norm)**

While not strictly a regularization technique in its original conception, **Batch Normalization (BN)** has been observed to have a regularizing effect. Its primary goal is to stabilize and accelerate the training process.

The core problem BN solves is called **Internal Covariate Shift**. As the weights in a layer are updated during training, the distribution of the inputs to the *next* layer is constantly changing. This forces the subsequent layer to continuously adapt to a new distribution, which slows down training.

Batch Norm addresses this by normalizing the inputs to each layer for each mini-batch. Specifically, for each feature in the mini-batch, it calculates the mean and variance and then normalizes the feature to have a **mean of 0 and a variance of 1**.
$$
\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}] + \epsilon}}
$$
However, strictly forcing a zero mean and unit variance might limit the layer's representational power. So, BN introduces two learnable parameters, gamma ($\gamma$) and beta ($\beta$), for each feature, which scale and shift the normalized output:
$$
y^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{(k)}
$$
This allows the network to learn the optimal distribution for each layer's input, which might not be zero mean and unit variance.

By stabilizing the input distributions, Batch Norm allows for higher learning rates, accelerates convergence, and acts as a regularizer, reducing the need for other techniques like Dropout. The noise introduced by the mini-batch statistics (since the mean and variance depend on the specific mini-batch) is thought to be the source of its regularizing effect.